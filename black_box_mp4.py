#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
MP4 ì˜ìƒ ì „ìš© ë¸”ë™ë°•ìŠ¤ ì‹œìŠ¤í…œ v2.3 - í™”ì‚´í‘œ í‚¤ ì™„ì „ ìˆ˜ì •íŒ
cv2.waitKeyEx()ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í”Œë«í¼ì—ì„œ íŠ¹ìˆ˜ í‚¤ ì…ë ¥ ì§€ì›
ì•ì°¨ ì¶œë°œ ì˜¤íƒ ë°©ì§€ ì™„ì „íŒ + ì‹œê°„ íƒìƒ‰ ê¸°ëŠ¥
"""

import cv2
import numpy as np
import time
import json
import logging
import os
import signal
import sys
import psutil
from datetime import datetime, timedelta
from collections import deque
import argparse
import queue
import threading
import hashlib
import math

from tts_config import LOGGING_LEVEL, LOGGING_FORMAT
from tts_settings import TTSNavigationSystem

# ì „ì—­ ë¡œê¹… ì„¤ì •
logging.basicConfig(level=getattr(logging, LOGGING_LEVEL), format=LOGGING_FORMAT)


class TimeNavigator:
    """ì‹œê°„ íƒìƒ‰ ê¸°ëŠ¥ í´ë˜ìŠ¤"""

    def __init__(self, logger):
        self.logger = logger
        self.jump_seconds = [5, 10, 30, 60, 120, 300]  # ì í”„ ê°€ëŠ¥í•œ ì´ˆ ë‹¨ìœ„
        self.current_jump_index = 2  # ê¸°ë³¸ 30ì´ˆ
        self.show_navigation_ui = False
        self.ui_show_time = 0
        self.ui_display_duration = 3.0  # UI í‘œì‹œ ì‹œê°„ (ì´ˆ)

    def get_current_jump_seconds(self):
        """í˜„ì¬ ì í”„ ì„¤ì •ê°’ ë°˜í™˜"""
        return self.jump_seconds[self.current_jump_index]

    def cycle_jump_time(self):
        """ì í”„ ì‹œê°„ ìˆœí™˜ (5ì´ˆ -> 10ì´ˆ -> 30ì´ˆ -> 60ì´ˆ -> 120ì´ˆ -> 300ì´ˆ -> 5ì´ˆ...)"""
        self.current_jump_index = (self.current_jump_index + 1) % len(self.jump_seconds)
        self.show_navigation_ui = True
        self.ui_show_time = time.time()
        jump_time = self.get_current_jump_seconds()
        self.logger.info(f"â±ï¸ ì í”„ ì‹œê°„ ì„¤ì •: {jump_time}ì´ˆ")
        return jump_time

    def should_show_ui(self):
        """UI í‘œì‹œ ì—¬ë¶€ í™•ì¸"""
        if not self.show_navigation_ui:
            return False
        if time.time() - self.ui_show_time > self.ui_display_duration:
            self.show_navigation_ui = False
            return False
        return True

    def format_time(self, seconds):
        """ì‹œê°„ì„ MM:SS í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
        if seconds < 0:
            return "00:00"
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes:02d}:{secs:02d}"


class VideoManager:
    def __init__(self, logger, config):
        self.logger = logger
        self.config = config
        self.video_capture = None
        self.video_fps = 30
        self.total_frames = 0
        self.current_frame_idx = 0
        self.video_duration = 0  # ì „ì²´ ì˜ìƒ ê¸¸ì´ (ì´ˆ)

    def init_video(self, video_path):
        """MP4 íŒŒì¼ ì´ˆê¸°í™”"""
        if not os.path.exists(video_path):
            self.logger.error(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return False

        self.logger.info(f"ğŸ“¹ MP4 íŒŒì¼ ë¡œë“œ ì¤‘: {video_path}")
        cap = cv2.VideoCapture(video_path)

        if not cap.isOpened():
            self.logger.error("âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False

        # ì²« í”„ë ˆì„ í™•ì¸
        ret, frame = cap.read()
        if not ret or frame is None or frame.size == 0:
            self.logger.error("âŒ ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            cap.release()
            return False

        # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        h, w = frame.shape[:2]
        ch = frame.shape[2] if len(frame.shape) == 3 else 1
        self.video_fps = cap.get(cv2.CAP_PROP_FPS)
        self.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.video_duration = self.total_frames / self.video_fps if self.video_fps > 0 else 0

        self.logger.info(
            f"âœ… ë¹„ë””ì˜¤ ì •ë³´: {w}x{h}x{ch}, {self.video_fps:.1f}fps, {self.video_duration:.1f}ì´ˆ, {self.total_frames}í”„ë ˆì„")

        # ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘
        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
        self.video_capture = cap
        self.current_frame_idx = 0
        return True

    def jump_to_time(self, target_seconds):
        """íŠ¹ì • ì‹œê°„(ì´ˆ)ìœ¼ë¡œ ì´ë™"""
        if not self.video_capture or not self.video_capture.isOpened():
            return False

        # ì‹œê°„ì„ í”„ë ˆì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        target_frame = int(target_seconds * self.video_fps)
        target_frame = max(0, min(target_frame, self.total_frames - 1))

        try:
            # í”„ë ˆì„ ìœ„ì¹˜ ì„¤ì •
            self.video_capture.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
            self.current_frame_idx = target_frame
            current_time = self.get_current_time()
            self.logger.info(f"â­ ì‹œê°„ ì´ë™: {current_time:.1f}ì´ˆ (í”„ë ˆì„ {target_frame})")
            return True
        except Exception as e:
            self.logger.error(f"ì‹œê°„ ì´ë™ ì‹¤íŒ¨: {e}")
            return False

    def jump_relative(self, seconds_delta):
        """í˜„ì¬ ìœ„ì¹˜ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ì´ë™ (ì•ìœ¼ë¡œ +, ë’¤ë¡œ -)"""
        current_time = self.get_current_time()
        target_time = current_time + seconds_delta

        # ë²”ìœ„ ì œí•œ
        target_time = max(0, min(target_time, self.video_duration))
        return self.jump_to_time(target_time)

    def get_current_time(self):
        """í˜„ì¬ ì¬ìƒ ì‹œê°„(ì´ˆ) ë°˜í™˜"""
        if self.video_fps == 0:
            return 0
        return self.current_frame_idx / self.video_fps

    def get_total_duration(self):
        """ì „ì²´ ì˜ìƒ ê¸¸ì´(ì´ˆ) ë°˜í™˜"""
        return self.video_duration

    def read_frame(self):
        """í”„ë ˆì„ ì½ê¸° (ë°˜ë³µ ì¬ìƒ ì§€ì›)"""
        if not self.video_capture or not self.video_capture.isOpened():
            return False, None

        ret, frame = self.video_capture.read()
        if ret and frame is not None:
            self.current_frame_idx += 1
            return True, frame
        else:
            # ë¹„ë””ì˜¤ ëì— ë„ë‹¬ - ë°˜ë³µ ì¬ìƒ
            loop_enabled = self.config.get('video_mode', {}).get('loop', True)
            if loop_enabled and self.total_frames > 0:
                self.logger.info("ğŸ”„ ë¹„ë””ì˜¤ ë°˜ë³µ ì¬ìƒ ì‹œì‘")
                self.video_capture.set(cv2.CAP_PROP_POS_FRAMES, 0)
                self.current_frame_idx = 0
                ret, frame = self.video_capture.read()
                if ret and frame is not None:
                    self.current_frame_idx = 1
                    return True, frame

            return False, None

    def get_fps(self):
        """FPS ë°˜í™˜"""
        return self.video_fps

    def get_progress(self):
        """ì¬ìƒ ì§„í–‰ë¥  ë°˜í™˜ (0.0 ~ 1.0)"""
        if self.total_frames == 0:
            return 0.0
        return min(self.current_frame_idx / self.total_frames, 1.0)

    def release(self):
        """ë¦¬ì†ŒìŠ¤ í•´ì œ"""
        if self.video_capture:
            self.video_capture.release()


class LaneDetector:
    """ì°¨ì„  ê°ì§€ ë° ë™ì  Front Zone ê³„ì‚°"""

    def __init__(self):
        self.lane_history = deque(maxlen=5)
        self.last_valid_lanes = None

    def detect_lanes(self, frame):
        height, width = frame.shape[:2]
        roi_vertices = np.array([[
            (0, height), (int(width * 0.5) - 50, int(height * 0.65)),
            (int(width * 0.5) + 50, int(height * 0.65)), (width, height)
        ]], dtype=np.int32)

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        blur = cv2.GaussianBlur(gray, (5, 5), 0)
        edges = cv2.Canny(blur, 50, 150)

        mask = np.zeros_like(edges)
        cv2.fillPoly(mask, roi_vertices, 255)
        masked_edges = cv2.bitwise_and(edges, mask)

        lines = cv2.HoughLinesP(masked_edges, rho=1, theta=np.pi / 180,
                                threshold=50, minLineLength=50, maxLineGap=50)
        return self.process_lane_lines(lines, width, height)

    def process_lane_lines(self, lines, width, height):
        if lines is None:
            return self.last_valid_lanes

        left_lines, right_lines = [], []
        for line in lines:
            x1, y1, x2, y2 = line[0]
            if x2 - x1 == 0:
                continue
            slope = (y2 - y1) / (x2 - x1)
            if slope < -0.5:
                left_lines.append((x1, y1, x2, y2))
            elif slope > 0.5:
                right_lines.append((x1, y1, x2, y2))

        left_lane = self.average_lane(left_lines, width, height)
        right_lane = self.average_lane(right_lines, width, height)

        if left_lane is not None or right_lane is not None:
            lanes = {'left': left_lane, 'right': right_lane}
            self.lane_history.append(lanes)
            self.last_valid_lanes = lanes
            return lanes

        return self.last_valid_lanes

    def average_lane(self, lane_lines, width, height):
        if not lane_lines:
            return None
        x_coords, y_coords = [], []
        for x1, y1, x2, y2 in lane_lines:
            x_coords.extend([x1, x2])
            y_coords.extend([y1, y2])
        if len(x_coords) < 2:
            return None
        poly = np.polyfit(y_coords, x_coords, 1)
        y1 = height
        y2 = int(height * 0.6)
        x1 = int(poly[0] * y1 + poly[1])
        x2 = int(poly[0] * y2 + poly[1])
        return [x1, y1, x2, y2]

    def calculate_lane_center_points(self, lanes, height):
        if not lanes or (lanes.get('left') is None and lanes.get('right') is None):
            return None
        center_points = []
        for y in range(int(height * 0.65), height, 20):
            left_x, right_x = None, None
            if lanes.get('left'):
                x1, y1, x2, y2 = lanes['left']
                if y2 != y1:
                    left_x = x1 + (x2 - x1) * (y - y1) / (y2 - y1)
            if lanes.get('right'):
                x1, y1, x2, y2 = lanes['right']
                if y2 != y1:
                    right_x = x1 + (x2 - x1) * (y - y1) / (y2 - y1)
            if left_x is not None and right_x is not None:
                center_x = (left_x + right_x) / 2
            elif left_x is not None:
                center_x = left_x + 60
            elif right_x is not None:
                center_x = right_x - 60
            else:
                continue
            center_points.append((int(center_x), y))
        return center_points


class ImprovedIOUTracker:
    """ê°œì„ ëœ IoU ê¸°ë°˜ ê°ì²´ ì¶”ì ê¸° - ì•ì°¨ ì¶œë°œ ì˜¤íƒ ë°©ì§€"""

    def __init__(self, max_lost=5, iou_threshold=0.3, movement_threshold=2, expected_fps=30):
        self.tracks = {}
        self.next_id = 1
        self.max_lost = max_lost
        self.iou_threshold = iou_threshold
        self.movement_threshold = movement_threshold
        self.frame_count = 0

        # âœ… ìƒˆë¡œìš´ ì¶œë°œ ê°ì§€ íŒŒë¼ë¯¸í„° (ì˜ìƒ ì†ë„ ì €í•˜ ëŒ€ì‘)
        self.expected_fps = expected_fps
        self.departure_buffer_frames = int(expected_fps * 2)  # 2ì´ˆ ìƒë‹¹ì˜ í”„ë ˆì„
        self.stationary_threshold = 1.5  # ì •ì§€ ìƒíƒœ íŒì • ì„ê³„ê°’
        self.min_stationary_ratio = 0.7  # ìµœì†Œ ì •ì§€ ë¹„ìœ¨ (70%)

    @staticmethod
    def calculate_iou(box1, box2):
        x1, y1, w1, h1 = box1
        x2, y2, w2, h2 = box2
        x_left = max(x1, x2)
        y_top = max(y1, y2)
        x_right = min(x1 + w1, x2 + w2)
        y_bottom = min(y1 + h1, y2 + h2)
        if x_right < x_left or y_bottom < y_top:
            return 0.0
        intersection = (x_right - x_left) * (y_bottom - y_top)
        area1 = w1 * h1
        area2 = w2 * h2
        union = area1 + area2 - intersection
        return intersection / union if union > 0 else 0.0

    def detect_departure_improved(self, track, is_in_zone=False):
        """âœ… ê°œì„ ëœ ì¶œë°œ ê°ì§€: ì‹œê°„ ê¸°ë°˜ ë²„í¼ë§ìœ¼ë¡œ ì˜¤íƒ ë°©ì§€"""
        if len(track['movement_history']) < self.departure_buffer_frames:
            return False

        # ìµœê·¼ Ní”„ë ˆì„ì˜ ì›€ì§ì„ ë¶„ì„
        recent_movements = list(track['movement_history'])[-self.departure_buffer_frames:]

        # 1ë‹¨ê³„: ì •ì§€ ìƒíƒœ íŒì • (ìµœê·¼ 10í”„ë ˆì„ ì œì™¸í•˜ê³  ë¶„ì„)
        analysis_frames = recent_movements[:-15] if len(recent_movements) > 15 else recent_movements[:-5]
        if not analysis_frames:
            return False

        stationary_count = 0
        for move_x, move_y in analysis_frames:
            distance = (move_x ** 2 + move_y ** 2) ** 0.5
            if distance < self.stationary_threshold:
                stationary_count += 1

        # 2ë‹¨ê³„: ì¶©ë¶„íˆ ì˜¤ë˜ ì •ì§€í–ˆëŠ”ì§€ í™•ì¸
        stationary_ratio = stationary_count / len(analysis_frames)
        if stationary_ratio < self.min_stationary_ratio:
            return False  # ì¶©ë¶„íˆ ì •ì§€í•˜ì§€ ì•Šì•˜ìŒ

        # 3ë‹¨ê³„: ìµœê·¼ í”„ë ˆì„ì—ì„œ ì‹¤ì œ ì›€ì§ì„ í™•ì¸
        recent_movement_frames = recent_movements[-10:] if len(recent_movements) >= 10 else recent_movements[-5:]
        avg_recent_distance = sum(
            (move_x ** 2 + move_y ** 2) ** 0.5
            for move_x, move_y in recent_movement_frames
        ) / len(recent_movement_frames)

        # 4ë‹¨ê³„: ì¶œë°œ íŒì • (ì˜ìƒ ì†ë„ ì €í•˜ë¥¼ ê³ ë ¤í•œ ì„ê³„ê°’)
        adjusted_threshold = self.movement_threshold * (2.0 if is_in_zone else 3.0)
        is_departed = avg_recent_distance > adjusted_threshold

        # 5ë‹¨ê³„: ë””ë²„ê·¸ ì •ë³´
        if is_departed:
            track['departure_info'] = {
                'stationary_ratio': stationary_ratio,
                'recent_movement': avg_recent_distance,
                'threshold_used': adjusted_threshold,
                'buffer_frames': len(recent_movements)
            }

        return is_departed

    def update(self, detections):
        self.frame_count += 1
        active_tracks = {tid: tr for tid, tr in self.tracks.items() if tr['lost'] <= self.max_lost}
        matched_tracks, matched_dets = set(), set()

        for track_id, track in active_tracks.items():
            best_iou, best_idx = 0.0, -1
            for i, det in enumerate(detections):
                if i in matched_dets:
                    continue
                iou = self.calculate_iou(track['bbox'], det['box'])
                if iou > best_iou and iou > self.iou_threshold:
                    best_iou, best_idx = iou, i
            if best_idx != -1:
                old_bbox = track['bbox']
                new_bbox = detections[best_idx]['box']
                old_cx = old_bbox[0] + old_bbox[2] / 2
                old_cy = old_bbox[1] + old_bbox[3] / 2
                new_cx = new_bbox[0] + new_bbox[2] / 2
                new_cy = new_bbox[1] + new_bbox[3] / 2
                movement_x = new_cx - old_cx
                movement_y = new_cy - old_cy

                track['bbox'] = new_bbox
                track['confidence'] = detections[best_idx]['confidence']
                track['class_name'] = detections[best_idx]['class_name']
                track['lost'] = 0
                track['movement_history'].append((movement_x, movement_y))
                track['last_update'] = self.frame_count

                is_in_zone = detections[best_idx].get('in_zone', False)

                # âœ… ê°œì„ ëœ ì¶œë°œ ê°ì§€ ì ìš©
                if self.detect_departure_improved(track, is_in_zone):
                    track['is_moving'] = True
                    track['departure_detected'] = True

                matched_tracks.add(track_id)
                matched_dets.add(best_idx)

        for track_id, track in active_tracks.items():
            if track_id not in matched_tracks:
                track['lost'] += 1

        # âœ… ë” ê¸´ ì›€ì§ì„ íˆìŠ¤í† ë¦¬ ìœ ì§€ (ì˜ìƒ ì†ë„ ì €í•˜ ëŒ€ì‘)
        max_history = max(self.departure_buffer_frames + 20, 100)
        for i, det in enumerate(detections):
            if i not in matched_dets:
                self.tracks[self.next_id] = {
                    'id': self.next_id,
                    'bbox': det['box'],
                    'confidence': det['confidence'],
                    'class_name': det['class_name'],
                    'lost': 0,
                    'created_frame': self.frame_count,
                    'last_update': self.frame_count,
                    'movement_history': deque(maxlen=max_history),  # ë” ê¸´ íˆìŠ¤í† ë¦¬
                    'is_moving': False,
                    'departure_detected': False,
                    'departure_info': {}
                }
                self.next_id += 1

        self.tracks = {tid: tr for tid, tr in self.tracks.items() if tr['lost'] <= self.max_lost}
        return self.get_active_tracks()

    def get_active_tracks(self):
        return [tr for tr in self.tracks.values() if tr['lost'] == 0]


class TrafficLightColorDetector:
    """ì‹ í˜¸ë“± ìƒ‰ìƒ ê°ì§€ê¸° - ì•ì°¨ ì¶œë°œ íŒë‹¨ìš©"""

    def __init__(self, stability_frames=2):
        self.stability_frames = stability_frames
        self.color_history = deque(maxlen=stability_frames)
        self.last_stable_color = None
        self.last_change_time = 0
        self.previous_color = None
        self.confidence_threshold = 0.5

    def detect_traffic_light_color(self, roi):
        if roi.size == 0:
            return None
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)

        red_lower1 = np.array([0, 100, 100]);
        red_upper1 = np.array([10, 255, 255])
        red_lower2 = np.array([170, 100, 100]);
        red_upper2 = np.array([180, 255, 255])
        red_lower3 = np.array([0, 80, 80]);
        red_upper3 = np.array([15, 255, 255])
        red_lower4 = np.array([165, 80, 80]);
        red_upper4 = np.array([180, 255, 255])
        yellow_lower = np.array([18, 120, 120]);
        yellow_upper = np.array([35, 255, 255])
        green_lower = np.array([45, 100, 100]);
        green_upper = np.array([90, 255, 255])

        red_mask = (cv2.inRange(hsv, red_lower1, red_upper1) +
                    cv2.inRange(hsv, red_lower2, red_upper2) +
                    cv2.inRange(hsv, red_lower3, red_upper3) +
                    cv2.inRange(hsv, red_lower4, red_upper4))
        yellow_mask = cv2.inRange(hsv, yellow_lower, yellow_upper)
        green_mask = cv2.inRange(hsv, green_lower, green_upper)

        red_pixels = cv2.countNonZero(red_mask)
        yellow_pixels = cv2.countNonZero(yellow_mask)
        green_pixels = cv2.countNonZero(green_mask)
        max_pixels = max(red_pixels, yellow_pixels, green_pixels)

        if max_pixels < 15:
            return None
        if red_pixels == max_pixels:
            return 'red'
        elif yellow_pixels == max_pixels:
            return 'yellow'
        elif green_pixels == max_pixels:
            return 'green'
        return None

    def update_color_history(self, color):
        if color:
            self.color_history.append(color)
            if len(self.color_history) >= self.stability_frames:
                counts = {}
                for c in self.color_history:
                    counts[c] = counts.get(c, 0) + 1
                most_common = max(counts, key=counts.get)
                confidence = counts[most_common] / self.stability_frames
                if confidence >= self.confidence_threshold:
                    if self.last_stable_color != most_common:
                        change_info = {
                            'datetime': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'timestamp': time.time()
                        }
                        self.previous_color = self.last_stable_color
                        self.last_stable_color = most_common
                        self.last_change_time = time.time()
                        return change_info
        return None


class MP4BlackBoxSystem:
    """MP4 ì˜ìƒ ì „ìš© ì•ì°¨ ì¶œë°œ ì˜¤íƒ ë°©ì§€ ë¸”ë™ë°•ìŠ¤ ì‹œìŠ¤í…œ + ì‹œê°„ íƒìƒ‰ ê¸°ëŠ¥"""

    def __init__(self, config_path="mp4_blackbox_config.json"):
        self.setup_logging()
        self.config_path = config_path
        self.config = self.load_config()

        self.video_manager = VideoManager(self.logger, self.config)
        self.time_navigator = TimeNavigator(self.logger)  # âœ… ì‹œê°„ íƒìƒ‰ ê¸°ëŠ¥ ì¶”ê°€
        self.net = None
        self.classes = []
        self.output_layers = []

        self.running = False
        self.current_frame = None
        self.video_writer = None
        self.paused = False  # âœ… ì¼ì‹œì •ì§€ ê¸°ëŠ¥

        # âœ… ê°œì„ ëœ ì¶”ì ê¸° ì ìš© (ì˜ìƒ FPS ê³ ë ¤)
        expected_fps = 30  # ê¸°ë³¸ FPS
        self.vehicle_tracker = ImprovedIOUTracker(
            max_lost=int(self.config.get('tracking', {}).get('max_lost_frames', 8)),
            iou_threshold=float(self.config.get('tracking', {}).get('iou_threshold', 0.25)),
            movement_threshold=float(self.config.get('tracking', {}).get('movement_threshold', 2)),
            expected_fps=expected_fps
        )
        self.traffic_light_detector = TrafficLightColorDetector(
            stability_frames=int(self.config.get('traffic_light', {}).get('stability_frames', 2))
        )
        self.lane_detector = LaneDetector()

        self.detection_stats = {
            'total_detections': 0,
            'vehicles': 0,
            'traffic_lights': 0,
            'traffic_light_changes': 0,
            'vehicle_departures': 0,
            'false_departures_prevented': 0  # âœ… ì˜¤íƒ ë°©ì§€ í†µê³„
        }

        self.frame_read_failures = 0
        self.max_frame_read_failures = 10

        # âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¶”ê°€
        self.processing_times = deque(maxlen=30)
        self.last_detection_time = 0

        self.logger.info("âœ… MP4 ì „ìš© ì•ì°¨ ì¶œë°œ ì˜¤íƒ ë°©ì§€ ë¸”ë™ë°•ìŠ¤ ì‹œìŠ¤í…œ + ì‹œê°„ íƒìƒ‰ ì´ˆê¸°í™” ì™„ë£Œ")

        # TTS ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            self.tts_system = TTSNavigationSystem()
            self.TTS_ALLOWED = ["ì‹ í˜¸ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤", "ì•ì˜ ì°¨ëŸ‰ì´ ì¶œë°œí•˜ì˜€ìŠµë‹ˆë‹¤"]
            self.tts_message_hashes = set()
        except Exception as e:
            self.logger.warning(f"TTS ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.tts_system = None

    def setup_logging(self):
        os.makedirs("log", exist_ok=True)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('log/mp4_blackbox.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def load_config(self):
        default_config = {
            "video": {
                "file_path": "blackbox.mp4",  # âœ… íŒŒì¼ëª… ì ìš©
                "output_dir": "output"
            },
            "video_mode": {
                "loop": True,
                "speed": 1.0,
                "save_result": True
            },
            "model": {
                "weights_path": "yolov4-tiny.weights",
                "config_path": "yolov4-tiny.cfg",
                "classes_path": "coco.names",
                "input_size": 416
            },
            "detection": {
                "confidence_threshold": 0.5,
                "nms_threshold": 0.4,
                "traffic_light_confidence_threshold": 0.3,
                "detection_interval": 2,  # âœ… ê°„ê²© ì¡°ì •ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
                "max_processing_time": 0.1
            },
            "traffic_light": {
                "enable_detection": True,
                "stability_frames": 2
            },
            "tracking": {
                "iou_threshold": 0.25,  # âœ… ë” ê´€ëŒ€í•œ ì¶”ì 
                "max_lost_frames": 8,  # âœ… ë” ì˜¤ë˜ ì¶”ì 
                "movement_threshold": 2,
                "departure_buffer_seconds": 2.5,  # âœ… ì¶œë°œ íŒì • ë²„í¼ ì‹œê°„
                "stationary_threshold": 1.5,
                "min_stationary_ratio": 0.7
            },
            "display": {
                "show_preview": True,
                "window_width": 1280,
                "window_height": 720
            }
        }

        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    cfg = json.load(f)

                def merge(a, b):
                    for k, v in b.items():
                        if k not in a:
                            a[k] = v
                        elif isinstance(v, dict):
                            merge(a[k], v)
                    return a

                return merge(cfg, default_config)
            else:
                with open(self.config_path, 'w', encoding='utf-8') as f:
                    json.dump(default_config, f, indent=2, ensure_ascii=False)
                return default_config
        except Exception as e:
            self.logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return default_config

    def init_video(self):
        video_path = self.config['video']['file_path']
        success = self.video_manager.init_video(video_path)

        # âœ… ì‹¤ì œ FPSë¥¼ ì¶”ì ê¸°ì— ë°˜ì˜
        if success:
            actual_fps = self.video_manager.get_fps()
            self.vehicle_tracker.expected_fps = actual_fps
            self.vehicle_tracker.departure_buffer_frames = int(actual_fps * 2.5)  # 2.5ì´ˆ ë²„í¼
            self.logger.info(
                f"âœ… ì‹¤ì œ ì˜ìƒ FPS: {actual_fps:.1f}, ì¶œë°œ íŒì • ë²„í¼: {self.vehicle_tracker.departure_buffer_frames}í”„ë ˆì„")

        return success

    def init_ai_model(self):
        try:
            weights_path = self.config['model']['weights_path']
            config_path = self.config['model']['config_path']
            classes_path = self.config['model']['classes_path']

            if not all(os.path.exists(p) for p in [weights_path, config_path, classes_path]):
                self.logger.warning("AI ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì²˜ë¦¬ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
                return False

            try:
                self.net = cv2.dnn.readNetFromDarknet(config_path, weights_path)
            except Exception:
                self.net = cv2.dnn.readNet(weights_path, config_path)

            self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
            self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)

            with open(classes_path, 'r', encoding='utf-8') as f:
                self.classes = [line.strip() for line in f if line.strip()]

            layer_names = self.net.getLayerNames()
            unconnected = self.net.getUnconnectedOutLayers()
            self.output_layers = [layer_names[i - 1] for i in unconnected]

            self.logger.info(f"âœ… AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {len(self.classes)}ê°œ í´ë˜ìŠ¤")
            return True

        except Exception as e:
            self.logger.error(f"AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def create_video_writer(self, frame_shape):
        try:
            if not self.config['video_mode'].get('save_result', True):
                return None, None

            output_dir = self.config['video']['output_dir']
            os.makedirs(output_dir, exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"MP4_arrow_key_fixed_{timestamp}.mp4"  # âœ… í™”ì‚´í‘œ í‚¤ ìˆ˜ì • ë²„ì „ í‘œì‹œ
            filepath = os.path.join(output_dir, filename)

            height, width = frame_shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            fps = self.video_manager.get_fps()

            if width % 2 != 0:
                width -= 1
            if height % 2 != 0:
                height -= 1

            writer = cv2.VideoWriter(filepath, fourcc, fps, (width, height))
            if writer.isOpened():
                self.logger.info(f"âœ… ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì‹œì‘: {filename}")
                return writer, filepath
            else:
                self.logger.error(f"âŒ ë¹„ë””ì˜¤ ë¼ì´í„° ìƒì„± ì‹¤íŒ¨")
                return None, None
        except Exception as e:
            self.logger.error(f"ë¹„ë””ì˜¤ ë¼ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return None, None

    def filter_vehicles_by_lane(self, vehicle_detections, frame_width, frame_height, frame):
        """ì°¨ì„  ê¸°ë°˜ ì•ì°¨ í•„í„°ë§ - ëª¨ë“  ì´ë™ì²´ í¬í•¨"""
        lanes = self.lane_detector.detect_lanes(frame)
        if lanes is None:
            return []

        center_points = self.lane_detector.calculate_lane_center_points(lanes, frame_height)
        if not center_points:
            return []

        filtered = []
        lane_width = 120  # ì°¨ì„  í­ í™•ëŒ€
        min_area = (frame_width * frame_height) * 0.008  # ìµœì†Œ ë©´ì  ì™„í™”

        for det in vehicle_detections:
            if det['class_name'] not in ['car', 'truck', 'bus', 'motorbike', 'bicycle']:
                continue

            x, y, w, h = det['box']
            vehicle_center_x = x + w / 2
            vehicle_center_y = y + h / 2

            # ì•ìª½ ì°¨ì„  ì˜ì—­ í™•ëŒ€
            if vehicle_center_y < frame_height * 0.6:
                continue

            closest_point = min(center_points, key=lambda p: abs(p[1] - vehicle_center_y))
            lane_center_x = closest_point[0]

            if abs(vehicle_center_x - lane_center_x) < lane_width / 2 and (w * h) > min_area:
                det['in_zone'] = True
                det['front_vehicle'] = True
                filtered.append(det)

        return filtered

    def detect_objects_optimized(self, frame):
        """âœ… ì„±ëŠ¥ ìµœì í™”ëœ ê°ì²´ ê°ì§€"""
        start_time = time.time()

        # ì²˜ë¦¬ ì‹œê°„ ì œí•œìœ¼ë¡œ ì„±ëŠ¥ ë³´ì¥
        max_processing_time = self.config['detection'].get('max_processing_time', 0.1)

        height, width = frame.shape[:2]
        input_size = int(self.config['model'].get('input_size', 416))

        blob = cv2.dnn.blobFromImage(frame, 0.00392, (input_size, input_size), (0, 0, 0), True, crop=False)
        self.net.setInput(blob)
        outputs = self.net.forward(self.output_layers)

        boxes, confidences, class_ids = [], [], []
        conf_th = float(self.config['detection']['confidence_threshold'])
        tl_conf_th = float(self.config['detection'].get('traffic_light_confidence_threshold', conf_th))
        nms_th = float(self.config['detection']['nms_threshold'])

        for output in outputs:
            # ì²˜ë¦¬ ì‹œê°„ ì²´í¬
            if time.time() - start_time > max_processing_time:
                break

            for detection in output:
                scores = detection[5:]
                if scores.size == 0:
                    continue
                class_id = int(np.argmax(scores))
                if class_id < 0 or class_id >= len(self.classes):
                    continue
                confidence = float(scores[class_id])
                class_name = self.classes[class_id]
                min_conf = tl_conf_th if class_name == 'traffic light' else conf_th
                if confidence > min_conf:
                    cx = int(detection[0] * width)
                    cy = int(detection[1] * height)
                    w = int(detection[2] * width)
                    h = int(detection[3] * height)
                    x = int(cx - w / 2)
                    y = int(cy - h / 2)
                    boxes.append([x, y, w, h])
                    confidences.append(confidence)
                    class_ids.append(class_id)

        indexes = cv2.dnn.NMSBoxes(boxes, confidences, conf_th, nms_th)
        detections, vehicle_dets, tl_dets = [], [], []

        if len(indexes) > 0:
            for i in indexes.flatten():
                class_name = self.classes[class_ids[i]]
                det = {
                    'class_name': class_name,
                    'confidence': float(confidences[i]),
                    'box': boxes[i],
                    'timestamp': time.time()
                }
                detections.append(det)

                if class_name == 'traffic light':
                    tl_dets.append(det)
                    self.detection_stats['traffic_lights'] += 1

                if class_name in ['car', 'truck', 'bus', 'motorbike', 'bicycle']:
                    vehicle_dets.append(det)
                    self.detection_stats['vehicles'] += 1

            self.detection_stats['total_detections'] += len(indexes)

        # ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
        processing_time = time.time() - start_time
        self.processing_times.append(processing_time)

        filtered_vehicle_dets = self.filter_vehicles_by_lane(vehicle_dets, width, height, frame)
        tracked_vehicles = self.vehicle_tracker.update(filtered_vehicle_dets) if filtered_vehicle_dets else []

        if tl_dets and self.config['traffic_light']['enable_detection']:
            self.analyze_traffic_light_colors_fast(frame, tl_dets)

        return detections, tracked_vehicles

    def analyze_traffic_light_colors_fast(self, frame, tl_detections):
        for det in tl_detections:
            x, y, w, h = det['box']
            roi = frame[max(0, y):min(frame.shape[0], y + h),
                  max(0, x):min(frame.shape[1], x + w)]
            if roi.size > 100:
                detected_color = self.traffic_light_detector.detect_traffic_light_color(roi)
                change = self.traffic_light_detector.update_color_history(detected_color)
                if change:
                    self.handle_traffic_light_change(change)

    def handle_traffic_light_change(self, change_info):
        log_message = "ì‹ í˜¸ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤"
        self.logger.info(f"ğŸš¦ {log_message}")
        self._tts_announce_if_needed(log_message)
        self.detection_stats['traffic_light_changes'] += 1

    def log_vehicle_departure(self, track):
        """âœ… ê°œì„ ëœ ì¶œë°œ ë¡œê·¸ (ë””ë²„ê·¸ ì •ë³´ í¬í•¨)"""
        if track.get('departure_detected', False):
            log_message = "ì•ì˜ ì°¨ëŸ‰ì´ ì¶œë°œí•˜ì˜€ìŠµë‹ˆë‹¤"

            # ë””ë²„ê·¸ ì •ë³´ í¬í•¨
            departure_info = track.get('departure_info', {})
            detailed_log = (f"ğŸš— ì•ì°¨ ì¶œë°œ: ID{track['id']} ({track['class_name']}) - "
                            f"ì •ì§€ë¹„ìœ¨: {departure_info.get('stationary_ratio', 0):.2f}, "
                            f"ìµœê·¼ì›€ì§ì„: {departure_info.get('recent_movement', 0):.2f}, "
                            f"ì„ê³„ê°’: {departure_info.get('threshold_used', 0):.2f}")

            self.logger.info(detailed_log)
            self.detection_stats['vehicle_departures'] += 1
            self._tts_announce_if_needed(log_message)

    def _tts_announce_if_needed(self, msg):
        if not self.tts_system:
            return
        msg_hash = hashlib.md5(msg.encode()).hexdigest()
        if msg in self.TTS_ALLOWED and msg_hash not in self.tts_message_hashes:
            try:
                self.tts_system.play_situation_from_txt(msg, keyword=msg)
                self.tts_message_hashes.add(msg_hash)
                if len(self.tts_message_hashes) > 1000:
                    self.tts_message_hashes.pop()
            except Exception as e:
                self.logger.error(f"TTS ì•ˆë‚´ ì‹¤íŒ¨: {e}")

    def draw_lane_overlay(self, overlay, lanes, center_points, width, height):
        if lanes is None:
            return
        # ì°¨ì„  ë¼ì¸
        if lanes.get('left'):
            x1, y1, x2, y2 = lanes['left']
            cv2.line(overlay, (x1, y1), (x2, y2), (0, 255, 0), 2)
        if lanes.get('right'):
            x1, y1, x2, y2 = lanes['right']
            cv2.line(overlay, (x1, y1), (x2, y2), (0, 255, 0), 2)
        if center_points:
            # ì¤‘ì•™ì„ ì„ ë…¸ë€ìƒ‰ìœ¼ë¡œ
            for i in range(len(center_points) - 1):
                cv2.line(overlay, center_points[i], center_points[i + 1], (0, 255, 255), 3)
            # ì°¨ì„  í­ í‘œì‹œ
            lane_width = 120
            for point in center_points:
                cx, cy = point
                cv2.rectangle(overlay, (cx - lane_width // 2, cy - 1), (cx + lane_width // 2, cy + 1),
                              (0, 255, 255), -1)

    def draw_time_navigation_overlay(self, overlay, width, height):
        """âœ… ê°œì„ ëœ ì‹œê°„ íƒìƒ‰ UI ì˜¤ë²„ë ˆì´ - ë” ëª…í™•í•œ í‚¤ ì•ˆë‚´"""
        if not self.time_navigator.should_show_ui():
            return

        # ë°˜íˆ¬ëª… ë°°ê²½
        nav_overlay = overlay.copy()
        nav_height = 160  # ë†’ì´ ì¦ê°€
        nav_y = height - nav_height - 50
        cv2.rectangle(nav_overlay, (50, nav_y), (width - 50, nav_y + nav_height), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, nav_overlay, 0.3, 0, overlay)

        # í…Œë‘ë¦¬
        cv2.rectangle(overlay, (50, nav_y), (width - 50, nav_y + nav_height), (0, 255, 255), 2)

        # ì œëª©
        cv2.putText(overlay, "TIME NAVIGATION CONTROLS", (60, nav_y + 25),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)

        # í˜„ì¬ ì‹œê°„ ì •ë³´
        current_time = self.video_manager.get_current_time()
        total_time = self.video_manager.get_total_duration()
        progress = self.video_manager.get_progress()
        time_text = f"Time: {self.time_navigator.format_time(current_time)} / {self.time_navigator.format_time(total_time)} ({progress * 100:.1f}%)"
        cv2.putText(overlay, time_text, (60, nav_y + 50),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        # ì í”„ ì‹œê°„ ì„¤ì • í‘œì‹œ
        jump_seconds = self.time_navigator.get_current_jump_seconds()
        jump_text = f"Jump Time: {jump_seconds}s"
        cv2.putText(overlay, jump_text, (60, nav_y + 75),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)

        # í‚¤ë³´ë“œ ì»¨íŠ¸ë¡¤ ì•ˆë‚´ (ë” ìƒì„¸íˆ)
        controls_text = [
            "[ARROW KEYS] Jump [A/D] 1s [1-6] Set jump time [SPACE] Pause",
            "[T] Cycle jump [Home/End] Start/End [+/-] Speed [H] Help [Q] Quit"
        ]

        for i, text in enumerate(controls_text):
            cv2.putText(overlay, text, (60, nav_y + 100 + i * 20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)

        # ì¼ì‹œì •ì§€ í‘œì‹œ
        if self.paused:
            cv2.putText(overlay, "PAUSED", (width - 150, nav_y + 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

        # í˜„ì¬ ì¬ìƒ ì†ë„ í‘œì‹œ
        speed = self.config['video_mode'].get('speed', 1.0)
        if speed != 1.0:
            speed_text = f"Speed: {speed:.1f}x"
            cv2.putText(overlay, speed_text, (width - 150, nav_y + 60),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)

    def draw_permanent_controls_hint(self, overlay, width, height):
        """í™”ë©´ í•˜ë‹¨ì— í•­ìƒ í‘œì‹œë˜ëŠ” ê¸°ë³¸ ì»¨íŠ¸ë¡¤ íŒíŠ¸"""
        hint_text = "CONTROLS: [ARROWS] Navigate [SPACE] Pause [T] Jump time [H] Help [Q] Quit"

        # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
        (text_w, text_h), _ = cv2.getTextSize(hint_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)

        # ë°˜íˆ¬ëª… ë°°ê²½
        hint_y = height - 25
        cv2.rectangle(overlay, (5, hint_y - text_h - 5), (text_w + 15, height - 5), (0, 0, 0), -1)

        # í…ìŠ¤íŠ¸ í‘œì‹œ
        cv2.putText(overlay, hint_text, (10, hint_y - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

    def draw_optimized_overlay(self, frame, detections, tracked_vehicles):
        """âœ… ê°œì„ ëœ ì˜¤ë²„ë ˆì´ - í•­ìƒ í‘œì‹œë˜ëŠ” ì»¨íŠ¸ë¡¤ íŒíŠ¸ ì¶”ê°€"""
        overlay = frame.copy()
        height, width = frame.shape[:2]

        # ì°¨ì„  ì˜¤ë²„ë ˆì´
        lanes = self.lane_detector.last_valid_lanes
        center_points = self.lane_detector.calculate_lane_center_points(lanes, height) if lanes else None
        self.draw_lane_overlay(overlay, lanes, center_points, width, height)

        # ê²€ì¶œëœ ì°¨ëŸ‰ë“¤ í‘œì‹œ
        for detection in detections:
            x, y, w, h = detection['box']
            class_name = detection['class_name']
            confidence = detection['confidence']

            if class_name in ['car', 'truck', 'bus', 'motorbike', 'bicycle']:
                vehicle_center_x = x + w / 2
                vehicle_center_y = y + h / 2

                if vehicle_center_y > height * 0.6:
                    is_in_lane = False
                    if center_points:
                        for cx, cy in center_points:
                            if abs(vehicle_center_y - cy) < 30:
                                lane_width = 120
                                if abs(vehicle_center_x - cx) < lane_width // 2:
                                    is_in_lane = True
                                    break

                    if is_in_lane:
                        colors = {
                            'car': (0, 255, 0),
                            'truck': (0, 255, 255),
                            'bus': (255, 255, 0),
                            'motorbike': (255, 0, 255),
                            'bicycle': (100, 255, 255)
                        }
                        color = colors.get(class_name, (0, 255, 0))
                        cv2.rectangle(overlay, (x, y), (x + w, y + h), color, 3)

                        conf_text = f"{class_name}: {confidence:.2f}"
                        cv2.putText(overlay, conf_text, (x, y - 5),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

            elif class_name == 'traffic light':
                color = (255, 255, 255)
                cv2.rectangle(overlay, (x, y), (x + w, y + h), color, 3)

                current_color = self.traffic_light_detector.last_stable_color
                if current_color:
                    signal_colors = {'red': (0, 0, 255), 'yellow': (0, 255, 255), 'green': (0, 255, 0)}
                    signal_color = signal_colors.get(current_color, (255, 255, 255))
                    cv2.circle(overlay, (x + w // 2, y - 20), 10, signal_color, -1)
                    cv2.circle(overlay, (x + w // 2, y - 20), 10, (255, 255, 255), 2)

        # ì¶”ì ëœ ì•ì°¨ í‘œì‹œ
        for tr in tracked_vehicles:
            x, y, w, h = tr['bbox']
            track_id = tr['id']
            class_name = tr['class_name']

            if tr.get('departure_detected', False):
                track_color = (0, 255, 255)
                status_text = "DEPARTED"
            elif tr.get('is_moving', False):
                track_color = (255, 255, 0)
                status_text = "MOVING"
            else:
                track_color = (0, 255, 0)
                status_text = "WAITING"

            cv2.rectangle(overlay, (x, y), (x + w, y + h), track_color, 4)

            id_text = f"FRONT-{track_id} ({status_text})"
            (text_w, text_h), _ = cv2.getTextSize(id_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
            cv2.rectangle(overlay, (x + 5, y + 5), (x + text_w + 15, y + text_h + 15), (0, 0, 0), -1)
            cv2.putText(overlay, id_text, (x + 10, y + text_h + 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, track_color, 2)

            history_len = len(tr.get('movement_history', []))
            buffer_needed = self.vehicle_tracker.departure_buffer_frames
            history_text = f"H:{history_len}/{buffer_needed}"
            cv2.putText(overlay, history_text, (x + 10, y + h - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, track_color, 1)

        # ìƒë‹¨ íŒ¨ë„
        panel_h = 50
        panel_overlay = overlay.copy()
        cv2.rectangle(panel_overlay, (0, 0), (width, panel_h), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.8, panel_overlay, 0.2, 0, overlay)
        cv2.line(overlay, (0, panel_h), (width, panel_h), (0, 255, 0), 2)

        # ì‹œê°„ ì •ë³´
        progress = self.video_manager.get_progress()
        current_time = self.video_manager.get_current_time()
        total_time = self.video_manager.get_total_duration()
        time_text = f"{self.time_navigator.format_time(current_time)} / {self.time_navigator.format_time(total_time)} ({progress * 100:.1f}%)"
        cv2.putText(overlay, time_text, (15, 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

        # ì„±ëŠ¥ ì •ë³´
        avg_processing_time = sum(self.processing_times) / len(self.processing_times) if self.processing_times else 0
        perf_text = f"PROC: {avg_processing_time * 1000:.1f}ms"
        cv2.putText(overlay, perf_text, (15, 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)

        # ì•ì°¨ ê°ì§€ ìƒíƒœ
        waiting_vehicles = len([t for t in tracked_vehicles if not t.get('is_moving', False)])
        moving_vehicles = len([t for t in tracked_vehicles if t.get('is_moving', False)])
        departed_vehicles = len([t for t in tracked_vehicles if t.get('departure_detected', False)])

        status_text = f"WAIT:{waiting_vehicles} MOVE:{moving_vehicles} DEPT:{departed_vehicles}"
        cv2.putText(overlay, status_text, (300, 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

        # ì¬ìƒ ì†ë„ í‘œì‹œ
        speed = self.config['video_mode'].get('speed', 1.0)
        speed_text = f"Speed: {speed:.1f}x"
        cv2.putText(overlay, speed_text, (300, 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)

        # ì‹ í˜¸ë“± ìƒíƒœ
        if self.traffic_light_detector.last_stable_color:
            signal_text = f"SIGNAL: {self.traffic_light_detector.last_stable_color.upper()}"
            cv2.putText(overlay, signal_text, (width - 200, 20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)

        # ì¼ì‹œì •ì§€ ìƒíƒœ
        if self.paused:
            cv2.putText(overlay, "PAUSED", (width - 200, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

        # ì‹œê°„ íƒìƒ‰ UI (í•„ìš”í•  ë•Œë§Œ)
        self.draw_time_navigation_overlay(overlay, width, height)

        # âœ… í•­ìƒ í‘œì‹œë˜ëŠ” ì»¨íŠ¸ë¡¤ íŒíŠ¸ ì¶”ê°€
        self.draw_permanent_controls_hint(overlay, width, height)

        return overlay

    def handle_keyboard_input(self, key):
        """âœ… waitKeyEx()ë¥¼ ì‚¬ìš©í•œ ì™„ì „í•œ í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬ - ëª¨ë“  í”Œë«í¼ ì§€ì›"""

        # í‚¤ê°€ ëˆŒë¦¬ì§€ ì•Šì€ ê²½ìš° (-1)ëŠ” ë¬´ì‹œ
        if key == -1:
            return False

        # ë””ë²„ê¹…ìš©: í‚¤ ì½”ë“œ í™•ì¸ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
        # print(f"Key pressed: {key} (0x{key:X})")

        # ê¸°ë³¸ ë¬¸ì í‚¤ë“¤
        if key == ord('q') or key == ord('Q'):
            self.logger.info("ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            self.running = False
            return True

        elif key == ord('s') or key == ord('S'):
            screenshot_name = f"arrow_key_fixed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg"
            if self.current_frame is not None:
                cv2.imwrite(screenshot_name, self.current_frame)
                self.logger.info(f"ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_name}")

        elif key == ord('t') or key == ord('T'):
            # ì í”„ ì‹œê°„ ìˆœí™˜
            jump_time = self.time_navigator.cycle_jump_time()

        elif key == 32:  # SPACE - ì¼ì‹œì •ì§€/ì¬ê°œ
            self.paused = not self.paused
            status = "ì¼ì‹œì •ì§€" if self.paused else "ì¬ê°œ"
            self.logger.info(f"â¸ï¸ ì˜ìƒ {status}")

        # âœ… í™”ì‚´í‘œ í‚¤ ì²˜ë¦¬ - Windows, Linux, macOS ëª¨ë“  í”Œë«í¼ ì§€ì›
        elif key in [2424832, 65361, 81]:  # LEFT ARROW (Windows, Linux, macOS)
            jump_seconds = self.time_navigator.get_current_jump_seconds()
            if self.video_manager.jump_relative(-jump_seconds):
                self.time_navigator.show_navigation_ui = True
                self.time_navigator.ui_show_time = time.time()
                self.logger.info(f"â¬…ï¸ {jump_seconds}ì´ˆ ë’¤ë¡œ ì í”„")

        elif key in [2555904, 65363, 83]:  # RIGHT ARROW (Windows, Linux, macOS)
            jump_seconds = self.time_navigator.get_current_jump_seconds()
            if self.video_manager.jump_relative(jump_seconds):
                self.time_navigator.show_navigation_ui = True
                self.time_navigator.ui_show_time = time.time()
                self.logger.info(f"â¡ï¸ {jump_seconds}ì´ˆ ì•ìœ¼ë¡œ ì í”„")

        elif key in [2490368, 65362, 82]:  # UP ARROW (Windows, Linux, macOS)
            if self.video_manager.jump_relative(10):
                self.time_navigator.show_navigation_ui = True
                self.time_navigator.ui_show_time = time.time()
                self.logger.info("â¬†ï¸ 10ì´ˆ ì•ìœ¼ë¡œ ì´ë™")

        elif key in [2621440, 65364, 84]:  # DOWN ARROW (Windows, Linux, macOS)
            if self.video_manager.jump_relative(-10):
                self.time_navigator.show_navigation_ui = True
                self.time_navigator.ui_show_time = time.time()
                self.logger.info("â¬‡ï¸ 10ì´ˆ ë’¤ë¡œ ì´ë™")

        elif key in [2359296, 65360, 71]:  # HOME (Windows, Linux, macOS)
            if self.video_manager.jump_to_time(0):
                self.time_navigator.show_navigation_ui = True
                self.time_navigator.ui_show_time = time.time()
                self.logger.info("ğŸ  ì˜ìƒ ì‹œì‘ìœ¼ë¡œ ì´ë™")

        elif key in [2293760, 65367, 79]:  # END (Windows, Linux, macOS)
            total_time = self.video_manager.get_total_duration()
            target_time = max(0, total_time - 10)
            if self.video_manager.jump_to_time(target_time):
                self.time_navigator.show_navigation_ui = True
                self.time_navigator.ui_show_time = time.time()
                self.logger.info("ğŸ”š ì˜ìƒ ëìœ¼ë¡œ ì´ë™")

        # ìˆ«ì í‚¤ë¡œ ì§ì ‘ ì í”„ ì‹œê°„ ì„¤ì •
        elif key == ord('1'):
            self.time_navigator.current_jump_index = 0  # 5ì´ˆ
            self.time_navigator.show_navigation_ui = True
            self.time_navigator.ui_show_time = time.time()
            self.logger.info(f"â±ï¸ ì í”„ ì‹œê°„: {self.time_navigator.get_current_jump_seconds()}ì´ˆ")

        elif key == ord('2'):
            self.time_navigator.current_jump_index = 1  # 10ì´ˆ
            self.time_navigator.show_navigation_ui = True
            self.time_navigator.ui_show_time = time.time()
            self.logger.info(f"â±ï¸ ì í”„ ì‹œê°„: {self.time_navigator.get_current_jump_seconds()}ì´ˆ")

        elif key == ord('3'):
            self.time_navigator.current_jump_index = 2  # 30ì´ˆ
            self.time_navigator.show_navigation_ui = True
            self.time_navigator.ui_show_time = time.time()
            self.logger.info(f"â±ï¸ ì í”„ ì‹œê°„: {self.time_navigator.get_current_jump_seconds()}ì´ˆ")

        elif key == ord('4'):
            self.time_navigator.current_jump_index = 3  # 60ì´ˆ
            self.time_navigator.show_navigation_ui = True
            self.time_navigator.ui_show_time = time.time()
            self.logger.info(f"â±ï¸ ì í”„ ì‹œê°„: {self.time_navigator.get_current_jump_seconds()}ì´ˆ")

        elif key == ord('5'):
            self.time_navigator.current_jump_index = 4  # 120ì´ˆ
            self.time_navigator.show_navigation_ui = True
            self.time_navigator.ui_show_time = time.time()
            self.logger.info(f"â±ï¸ ì í”„ ì‹œê°„: {self.time_navigator.get_current_jump_seconds()}ì´ˆ")

        elif key == ord('6'):
            self.time_navigator.current_jump_index = 5  # 300ì´ˆ
            self.time_navigator.show_navigation_ui = True
            self.time_navigator.ui_show_time = time.time()
            self.logger.info(f"â±ï¸ ì í”„ ì‹œê°„: {self.time_navigator.get_current_jump_seconds()}ì´ˆ")

        # A/D í‚¤ë¡œ ë¯¸ì„¸ ì¡°ì •
        elif key == ord('a') or key == ord('A'):
            if self.video_manager.jump_relative(-1):
                self.time_navigator.show_navigation_ui = True
                self.time_navigator.ui_show_time = time.time()
                self.logger.info("â¬…ï¸ 1ì´ˆ ë’¤ë¡œ")

        elif key == ord('d') or key == ord('D'):
            if self.video_manager.jump_relative(1):
                self.time_navigator.show_navigation_ui = True
                self.time_navigator.ui_show_time = time.time()
                self.logger.info("â¡ï¸ 1ì´ˆ ì•ìœ¼ë¡œ")

        # ì¬ìƒ ì†ë„ ì¡°ì ˆ
        elif key == ord('+') or key == ord('='):
            current_speed = self.config['video_mode'].get('speed', 1.0)
            new_speed = min(current_speed * 1.5, 4.0)
            self.config['video_mode']['speed'] = new_speed
            self.logger.info(f"âš¡ ì¬ìƒ ì†ë„: {new_speed:.1f}x")

        elif key == ord('-') or key == ord('_'):
            current_speed = self.config['video_mode'].get('speed', 1.0)
            new_speed = max(current_speed / 1.5, 0.25)
            self.config['video_mode']['speed'] = new_speed
            self.logger.info(f"ğŸŒ ì¬ìƒ ì†ë„: {new_speed:.1f}x")

        # ë„ì›€ë§ í‘œì‹œ
        elif key == ord('h') or key == ord('H'):
            self.show_help()

        return False  # ì¢…ë£Œí•˜ì§€ ì•ŠìŒ

    def show_help(self):
        """í‚¤ë³´ë“œ ì»¨íŠ¸ë¡¤ ë„ì›€ë§ í‘œì‹œ"""
        help_text = """
==================== í‚¤ë³´ë“œ ì»¨íŠ¸ë¡¤ ====================
[ê¸°ë³¸ ì œì–´]
Q - í”„ë¡œê·¸ë¨ ì¢…ë£Œ
SPACE - ì¼ì‹œì •ì§€/ì¬ê°œ
S - ìŠ¤í¬ë¦°ìƒ· ì €ì¥
H - ì´ ë„ì›€ë§ í‘œì‹œ

[ì‹œê°„ íƒìƒ‰] âœ… í™”ì‚´í‘œ í‚¤ ì™„ì „ ìˆ˜ì •ë¨
â†â†’ - ì„¤ì •ëœ ì‹œê°„ë§Œí¼ ì í”„ (ê¸°ë³¸ 30ì´ˆ)
â†‘â†“ - 10ì´ˆ ì í”„
A/D - 1ì´ˆ ë¯¸ì„¸ ì¡°ì •
Home - ì˜ìƒ ì‹œì‘ìœ¼ë¡œ
End - ì˜ìƒ ëìœ¼ë¡œ

[ì í”„ ì‹œê°„ ì„¤ì •]
T - ì í”„ ì‹œê°„ ìˆœí™˜ (5â†’10â†’30â†’60â†’120â†’300ì´ˆ)
1 - 5ì´ˆ ì í”„ë¡œ ì„¤ì •
2 - 10ì´ˆ ì í”„ë¡œ ì„¤ì •
3 - 30ì´ˆ ì í”„ë¡œ ì„¤ì •
4 - 60ì´ˆ ì í”„ë¡œ ì„¤ì •
5 - 120ì´ˆ ì í”„ë¡œ ì„¤ì •
6 - 300ì´ˆ ì í”„ë¡œ ì„¤ì •

[ì¬ìƒ ì†ë„]
+/= - ì¬ìƒ ì†ë„ ì¦ê°€ (ìµœëŒ€ 4ë°°ì†)
-/_ - ì¬ìƒ ì†ë„ ê°ì†Œ (ìµœì†Œ 0.25ë°°ì†)
====================================================
"""
        self.logger.info(help_text)
        print(help_text)

    def signal_handler(self, signum, frame):
        self.running = False
        self.cleanup_resources()
        sys.exit(0)

    def cleanup_resources(self):
        try:
            if hasattr(self, 'video_writer') and self.video_writer:
                self.video_writer.release()
            self.video_manager.release()
            cv2.destroyAllWindows()
            if hasattr(self, 'tts_system') and self.tts_system:
                self.tts_system.shutdown()
        except Exception as e:
            self.logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def run(self):
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)

        if not self.init_video():
            return False

        ai_ok = self.init_ai_model()
        if not ai_ok:
            self.logger.warning("AI ëª¨ë¸ ì—†ì´ ê¸°ë³¸ ì²˜ë¦¬ ëª¨ë“œë¡œ ì‹¤í–‰")

        self.running = True
        frame_count = 0
        last_fps_time = time.time()
        fps_counter = 0

        # ì²« í”„ë ˆì„ìœ¼ë¡œ Writer ì¤€ë¹„
        ok, first = self.video_manager.read_frame()
        if not ok or first is None:
            self.logger.error("ì´ˆê¸° í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.cleanup_resources()
            return False

        self.video_writer, video_path = self.create_video_writer(first.shape)

        # ë¯¸ë¦¬ë³´ê¸° ì°½ ì„¤ì •
        if self.config['display'].get('show_preview', True):
            cv2.namedWindow("MP4 Arrow Key Fixed Player", cv2.WINDOW_NORMAL)
            window_w = self.config['display'].get('window_width', 1280)
            window_h = self.config['display'].get('window_height', 720)
            cv2.resizeWindow("MP4 Arrow Key Fixed Player", window_w, window_h)

        # ë¹„ë””ì˜¤ ì¬ìƒ ì†ë„ ì¡°ì ˆ
        video_fps = self.video_manager.get_fps()
        frame_delay = 1.0 / video_fps if video_fps > 0 else 1.0 / 30
        speed_multiplier = self.config['video_mode'].get('speed', 1.0)
        frame_delay = frame_delay / speed_multiplier

        total_duration = self.video_manager.get_total_duration()
        self.logger.info(
            f"ğŸ¬ MP4 í™”ì‚´í‘œ í‚¤ ìˆ˜ì • ì™„ë£Œ + ì•ì°¨ ê°ì§€ ì‹œì‘ (FPS: {video_fps:.1f}, ê¸¸ì´: {self.time_navigator.format_time(total_duration)}, ì†ë„: {speed_multiplier}x)")
        self.logger.info("âœ… í™”ì‚´í‘œ í‚¤ ì‘ë™ í™•ì¸: [â†â†’]ì í”„ [â†‘â†“]Â±10ì´ˆ [T]ì í”„ì„¤ì • [Space]ì¼ì‹œì •ì§€ [Q]ì¢…ë£Œ")

        while self.running:
            try:
                if not self.paused:
                    ret, frame = self.video_manager.read_frame()
                    if not ret or frame is None:
                        if self.config['video_mode'].get('loop', True):
                            continue
                        else:
                            self.logger.info("ğŸ“¹ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ")
                            break

                    self.current_frame = frame.copy()

                    detections, tracked_vehicles = [], []
                    detection_interval = int(self.config['detection']['detection_interval'])

                    if ai_ok and self.net is not None:
                        if frame_count % detection_interval == 0:
                            detections, tracked_vehicles = self.detect_objects_optimized(frame)

                    overlay_frame = self.draw_optimized_overlay(frame.copy(), detections, tracked_vehicles)

                    # ì•ì°¨ ì¶œë°œ ê°ì§€ ë¡œê·¸
                    for track in tracked_vehicles:
                        if track.get('departure_detected', False) and not track.get('logged', False):
                            self.log_vehicle_departure(track)
                            track['logged'] = True

                    # ê²°ê³¼ ë¹„ë””ì˜¤ ì €ì¥
                    if self.video_writer and self.video_writer.isOpened():
                        try:
                            h, w = overlay_frame.shape[:2]
                            if w % 2 != 0:
                                overlay_frame = overlay_frame[:, :-1]
                            if h % 2 != 0:
                                overlay_frame = overlay_frame[:-1, :]
                            self.video_writer.write(overlay_frame)
                        except Exception as e:
                            self.logger.error(f"í”„ë ˆì„ ì €ì¥ ì‹¤íŒ¨: {e}")

                    frame_count += 1

                else:
                    # ì¼ì‹œì •ì§€ ìƒíƒœì—ì„œëŠ” í˜„ì¬ í”„ë ˆì„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    if self.current_frame is not None:
                        overlay_frame = self.draw_optimized_overlay(self.current_frame.copy(), [], [])
                    else:
                        time.sleep(0.1)
                        continue

                # âœ… ì‹¤ì‹œê°„ ë¯¸ë¦¬ë³´ê¸° ì°½ í‘œì‹œ - waitKeyEx() ì‚¬ìš©
                if self.config['display'].get('show_preview', True):
                    cv2.imshow("MP4 Arrow Key Fixed Player", overlay_frame)

                    # âœ… waitKeyEx()ë¡œ ë³€ê²½í•˜ì—¬ í™”ì‚´í‘œ í‚¤ ì™„ì „ ì§€ì›
                    key = cv2.waitKeyEx(1)

                    if self.handle_keyboard_input(key):
                        break

                # FPS ë° í†µê³„ (ê°œì„ ëœ ì •ë³´)
                if not self.paused:
                    fps_counter += 1
                    if fps_counter % 30 == 0:
                        current_time = time.time()
                        fps = 30 / (current_time - last_fps_time) if (current_time - last_fps_time) > 0 else 0
                        last_fps_time = current_time
                        active_tracks = len([])  # tracked_vehiclesëŠ” ì¼ì‹œì •ì§€ì‹œì—ë„ í‘œì‹œë˜ë¯€ë¡œ
                        waiting_vehicles = 0
                        moving_vehicles = 0
                        progress = self.video_manager.get_progress()
                        avg_proc_time = sum(self.processing_times) / len(
                            self.processing_times) if self.processing_times else 0
                        current_video_time = self.video_manager.get_current_time()

                        self.logger.info(
                            f"ğŸ“Š ì‹œê°„: {self.time_navigator.format_time(current_video_time)} | ì§„í–‰ë¥ : {progress * 100:.1f}% | FPS: {fps:.1f} | "
                            f"ì²˜ë¦¬ì‹œê°„: {avg_proc_time * 1000:.1f}ms | ì¶œë°œ: {self.detection_stats['vehicle_departures']} | "
                            f"ì‹ í˜¸ë“±: {self.detection_stats['traffic_lights']}, ë³€í™”: {self.detection_stats['traffic_light_changes']}"
                        )

                    # ì¬ìƒ ì†ë„ ì¡°ì ˆ (ì¼ì‹œì •ì§€ê°€ ì•„ë‹ ë•Œë§Œ)
                    time.sleep(frame_delay)

            except KeyboardInterrupt:
                self.logger.info("Keyboard interrupt ê°ì§€")
                break
            except Exception as e:
                self.logger.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                break

        self.cleanup_resources()
        return True


def main():
    parser = argparse.ArgumentParser(description='MP4 ì „ìš© í™”ì‚´í‘œ í‚¤ ì™„ì „ ìˆ˜ì • ë¸”ë™ë°•ìŠ¤ ì‹œìŠ¤í…œ')
    parser.add_argument('--config', default='mp4_blackbox_config.json', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--video', help='ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ì„¤ì • íŒŒì¼ë³´ë‹¤ ìš°ì„ )')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ëª¨ë“œ')
    args = parser.parse_args()

    try:
        blackbox = MP4BlackBoxSystem(args.config)

        # ëª…ë ¹í–‰ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ì´ ì§€ì •ëœ ê²½ìš°
        if args.video:
            blackbox.config['video']['file_path'] = args.video

        success = blackbox.run()
        return 0 if success else 1
    except KeyboardInterrupt:
        return 0
    except Exception as e:
        if args.debug:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
