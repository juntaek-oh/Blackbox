#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
YouTube ë§í¬ ì§ì ‘ ì²˜ë¦¬ ë¸”ë™ë°•ìŠ¤ ì‹œìŠ¤í…œ v3.3 - í•œê¸€ í°íŠ¸ ê¹¨ì§ ìˆ˜ì • + UI ê°„ì†Œí™”íŒ
í•œê¸€ í…ìŠ¤íŠ¸ ì™„ë²½ í‘œì‹œ + YouTube ìŠ¤íŠ¸ë¦¼ ì „ìš© ìµœì í™”
"""

import cv2
import numpy as np
import time
import json
import logging
import os
import signal
import sys
import psutil
from datetime import datetime, timedelta
from collections import deque
import argparse
import queue
import threading
import hashlib
import math
import requests
import subprocess

# YouTube ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import yt_dlp

    YT_DLP_AVAILABLE = True
except ImportError:
    YT_DLP_AVAILABLE = False
    print("âš ï¸ yt-dlp ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install yt-dlp' ì‹¤í–‰ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")

from tts_config import LOGGING_LEVEL, LOGGING_FORMAT
from tts_settings import TTSNavigationSystem

# ì „ì—­ ë¡œê¹… ì„¤ì •
logging.basicConfig(level=getattr(logging, LOGGING_LEVEL), format=LOGGING_FORMAT)


class SmartVideoSizeManager:
    """ìŠ¤ë§ˆíŠ¸ ì˜ìƒ í¬ê¸° ìë™ ê°ì§€ ë° ì¡°ì • ê´€ë¦¬ì"""

    def __init__(self, logger):
        self.logger = logger
        self.original_width = 1280
        self.original_height = 720
        self.display_width = 1280
        self.display_height = 720
        self.aspect_ratio = 16 / 9
        self.scale_factor = 1.0
        self.max_window_width = 1920
        self.max_window_height = 1080
        self.min_window_width = 640
        self.min_window_height = 360

    def update_video_size(self, frame_width, frame_height):
        """ì˜ìƒì˜ ì‹¤ì œ í¬ê¸°ì— ë§ê²Œ í‘œì‹œ í¬ê¸° ì¡°ì •"""
        self.original_width = frame_width
        self.original_height = frame_height
        self.aspect_ratio = frame_width / frame_height if frame_height > 0 else 16 / 9

        # ìµœì  í‘œì‹œ í¬ê¸° ê³„ì‚°
        self.calculate_optimal_display_size()

        self.logger.info(f"âœ… ì˜ìƒ í¬ê¸° ê°ì§€: {frame_width}x{frame_height} (ë¹„ìœ¨: {self.aspect_ratio:.3f})")
        self.logger.info(f"âœ… í‘œì‹œ í¬ê¸° ì¡°ì •: {self.display_width}x{self.display_height} (ë°°ìœ¨: {self.scale_factor:.2f}x)")

    def calculate_optimal_display_size(self):
        """í™”ë©´ í¬ê¸°ì— ë§ëŠ” ìµœì  í‘œì‹œ í¬ê¸° ê³„ì‚°"""
        if self.original_width > self.max_window_width or self.original_height > self.max_window_height:
            width_scale = self.max_window_width / self.original_width
            height_scale = self.max_window_height / self.original_height
            self.scale_factor = min(width_scale, height_scale)
        elif self.original_width < self.min_window_width or self.original_height < self.min_window_height:
            width_scale = self.min_window_width / self.original_width
            height_scale = self.min_window_height / self.original_height
            self.scale_factor = max(width_scale, height_scale)
        else:
            self.scale_factor = 1.0

        self.display_width = int(self.original_width * self.scale_factor)
        self.display_height = int(self.original_height * self.scale_factor)

        if self.display_width % 2 != 0:
            self.display_width += 1
        if self.display_height % 2 != 0:
            self.display_height += 1

        if self.display_width > self.max_window_width:
            self.display_width = self.max_window_width
            self.display_height = int(self.display_width / self.aspect_ratio)
        if self.display_height > self.max_window_height:
            self.display_height = self.max_window_height
            self.display_width = int(self.display_height * self.aspect_ratio)

    def resize_frame_if_needed(self, frame):
        """í•„ìš”í•œ ê²½ìš° í”„ë ˆì„ í¬ê¸° ì¡°ì •"""
        if frame is None:
            return None

        current_height, current_width = frame.shape[:2]

        if current_width != self.display_width or current_height != self.display_height:
            if self.scale_factor < 1.0:
                resized_frame = cv2.resize(frame, (self.display_width, self.display_height),
                                           interpolation=cv2.INTER_AREA)
            else:
                resized_frame = cv2.resize(frame, (self.display_width, self.display_height),
                                           interpolation=cv2.INTER_CUBIC)
            return resized_frame
        return frame

    def get_window_size(self):
        """ìœˆë„ìš° í¬ê¸° ë°˜í™˜"""
        return self.display_width, self.display_height


class KoreanTextRenderer:
    """âœ… í•œê¸€ í…ìŠ¤íŠ¸ ì™„ë²½ ë Œë”ë§ í´ë˜ìŠ¤"""

    def __init__(self):
        self.font_loaded = False
        self.font_scale = 0.5
        self.thickness = 1

    def put_korean_text(self, img, text, position, font_scale=0.5, color=(255, 255, 255), thickness=1, background=None):
        """í•œê¸€ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ë¥¼ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸° (OpenCV ê¸°ë³¸ í°íŠ¸ ì‚¬ìš© + ì˜ë¬¸ ëŒ€ì²´)"""
        try:
            # í•œê¸€ì´ í¬í•¨ëœ ê²½ìš° ì˜ë¬¸ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ í‘œì‹œ
            clean_text = self.convert_korean_to_english(text)

            # ë°°ê²½ìƒ‰ì´ ì§€ì •ëœ ê²½ìš° ë°°ê²½ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            if background is not None:
                (text_w, text_h), _ = cv2.getTextSize(clean_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
                x, y = position
                cv2.rectangle(img, (x - 2, y - text_h - 5), (x + text_w + 5, y + 5), background, -1)

            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
            cv2.putText(img, clean_text, position, cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)

        except Exception as e:
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ OpenCV í°íŠ¸ë¡œ ì˜ë¬¸ë§Œ í‘œì‹œ
            clean_text = self.extract_english_only(text)
            cv2.putText(img, clean_text, position, cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)

    def convert_korean_to_english(self, text):
        """í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ìˆëŠ” ì˜ë¬¸ìœ¼ë¡œ ë³€í™˜"""
        korean_to_english = {
            # ê¸°ë³¸ ìƒíƒœ
            "ëŒ€ê¸°": "WAIT",
            "ì´ë™": "MOVE",
            "ì¶œë°œ": "DEPT",
            "ê²€ì¶œ": "DETECT",
            "ì°¨ëŸ‰": "VEHICLE",
            "ì¶”ì ": "TRACK",
            "ì‹ í˜¸ë“±": "SIGNAL",
            "ë³€í™”": "CHANGE",
            "ì›ë³¸": "ORIG",
            "í‘œì‹œ": "DISP",

            # ì‹ í˜¸ë“± ìƒ‰ìƒ
            "ë¹¨ê°„ìƒ‰": "RED",
            "ë…¸ë€ìƒ‰": "YELLOW",
            "ì´ˆë¡ìƒ‰": "GREEN",
            "ì ìƒ‰": "RED",
            "í™©ìƒ‰": "YELLOW",
            "ë…¹ìƒ‰": "GREEN",

            # ì‹œê°„ ê´€ë ¨
            "ì‹œê°„": "TIME",
            "ì´ˆ": "SEC",
            "ë¶„": "MIN",
            "ì‹œ": "HOUR",

            # ê¸°íƒ€
            "ì‹¤ì‹œê°„": "LIVE",
            "ì—°ê²°": "CONN",
            "ìƒíƒœ": "STATUS",
            "ì •ë³´": "INFO",
            "ì‹œìŠ¤í…œ": "SYSTEM"
        }

        result = text
        for korean, english in korean_to_english.items():
            result = result.replace(korean, english)

        return result

    def extract_english_only(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¬¸, ìˆ«ì, ê¸°í˜¸ë§Œ ì¶”ì¶œ"""
        import re
        return re.sub(r'[^\x00-\x7F]', '', text)


class YouTubeVideoManager:
    """YouTube ë§í¬ ì²˜ë¦¬ ë° ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ê´€ë¦¬"""

    def __init__(self, logger, config):
        self.logger = logger
        self.config = config
        self.video_capture = None
        self.video_fps = 30
        self.total_frames = 0
        self.current_frame_idx = 0
        self.video_duration = 0
        self.youtube_url = None
        self.stream_url = None
        self.video_info = {}

        # í¬ê¸° ê´€ë¦¬ì ì¶”ê°€
        self.size_manager = SmartVideoSizeManager(logger)
        self.frame_size_detected = False

    def init_youtube_video(self, youtube_url):
        """YouTube URLì—ì„œ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™”"""
        if not YT_DLP_AVAILABLE:
            self.logger.error("âŒ yt-dlp ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return False

        self.youtube_url = youtube_url
        self.logger.info(f"ğŸ“¹ YouTube ì˜ìƒ ë¡œë“œ ì¤‘: {youtube_url}")

        try:
            quality_formats = [
                'best[height<=1080][ext=mp4]/best[height<=1080]',
                'best[height<=720][ext=mp4]/best[height<=720]',
                'best[height<=480][ext=mp4]/best[height<=480]',
                'best[height<=360][ext=mp4]/best[height<=360]',
                'best[ext=mp4]/best',
                'worst'
            ]

            best_info = None
            selected_format = None

            for format_selector in quality_formats:
                try:
                    ydl_opts = {
                        'format': format_selector,
                        'quiet': True,
                        'no_warnings': True,
                        'extract_flat': False,
                    }

                    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                        self.video_info = ydl.extract_info(youtube_url, download=False)

                        if 'url' in self.video_info:
                            self.stream_url = self.video_info['url']
                            best_info = self.video_info
                            selected_format = format_selector
                            break
                        elif 'formats' in self.video_info:
                            for fmt in self.video_info['formats']:
                                if (fmt.get('vcodec') != 'none' and
                                        fmt.get('acodec') != 'none' and
                                        'url' in fmt):
                                    self.stream_url = fmt['url']
                                    best_info = self.video_info
                                    selected_format = format_selector
                                    if 'width' in fmt and 'height' in fmt:
                                        self.video_info['detected_width'] = fmt['width']
                                        self.video_info['detected_height'] = fmt['height']
                                    break
                            if self.stream_url:
                                break

                except Exception as e:
                    self.logger.warning(f"í’ˆì§ˆ '{format_selector}' ì‹œë„ ì‹¤íŒ¨: {e}")
                    continue

            if not self.stream_url:
                self.logger.error("âŒ ì ì ˆí•œ ìŠ¤íŠ¸ë¦¼ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False

            self.video_duration = self.video_info.get('duration', 0)
            title = self.video_info.get('title', 'Unknown')
            uploader = self.video_info.get('uploader', 'Unknown')

            detected_width = self.video_info.get('detected_width', 'Unknown')
            detected_height = self.video_info.get('detected_height', 'Unknown')

            self.logger.info(f"âœ… YouTube ì˜ìƒ ì •ë³´:")
            self.logger.info(f"   ì œëª©: {title[:50]}{'...' if len(title) > 50 else ''}")
            self.logger.info(f"   ì—…ë¡œë”: {uploader}")
            self.logger.info(f"   ê¸¸ì´: {self.video_duration:.1f}ì´ˆ")
            self.logger.info(f"   ì„ íƒëœ í’ˆì§ˆ: {selected_format}")
            self.logger.info(f"   ì˜ˆìƒ í•´ìƒë„: {detected_width}x{detected_height}")

        except Exception as e:
            self.logger.error(f"âŒ YouTube ì˜ìƒ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return False

        # OpenCV VideoCaptureë¡œ ìŠ¤íŠ¸ë¦¼ ì—´ê¸°
        try:
            self.logger.info("ğŸ“¡ ìŠ¤íŠ¸ë¦¼ ì—°ê²° ì¤‘...")
            cap = cv2.VideoCapture(self.stream_url)

            max_retries = 3
            for attempt in range(max_retries):
                if cap.isOpened():
                    break
                self.logger.warning(f"ì—°ê²° ì‹œë„ {attempt + 1}/{max_retries}...")
                time.sleep(2)
                cap.release()
                cap = cv2.VideoCapture(self.stream_url)

            if not cap.isOpened():
                self.logger.error("âŒ ìŠ¤íŠ¸ë¦¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False

            max_frame_attempts = 10
            frame = None
            for attempt in range(max_frame_attempts):
                ret, frame = cap.read()
                if ret and frame is not None and frame.size > 0:
                    break
                self.logger.warning(f"í”„ë ˆì„ ì½ê¸° ì‹œë„ {attempt + 1}/{max_frame_attempts}...")
                time.sleep(0.5)

            if frame is None:
                self.logger.error("âŒ ìŠ¤íŠ¸ë¦¼ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                cap.release()
                return False

            actual_height, actual_width = frame.shape[:2]
            ch = frame.shape[2] if len(frame.shape) == 3 else 1

            self.size_manager.update_video_size(actual_width, actual_height)
            self.frame_size_detected = True

            self.video_fps = cap.get(cv2.CAP_PROP_FPS)

            if self.video_fps <= 0 or self.video_fps > 60:
                self.video_fps = 30
                self.logger.info(f"âš ï¸ FPS ìë™ ì„¤ì •: {self.video_fps}fps")

            self.total_frames = int(self.video_duration * self.video_fps) if self.video_duration > 0 else 999999

            self.logger.info(f"âœ… ìŠ¤íŠ¸ë¦¼ ì—°ê²° ì„±ê³µ:")
            self.logger.info(f"   ì‹¤ì œ í•´ìƒë„: {actual_width}x{actual_height}x{ch}")
            self.logger.info(f"   í‘œì‹œ í•´ìƒë„: {self.size_manager.display_width}x{self.size_manager.display_height}")
            self.logger.info(f"   FPS: {self.video_fps:.1f}")

            self.video_capture = cap
            self.current_frame_idx = 0
            return True

        except Exception as e:
            self.logger.error(f"ìŠ¤íŠ¸ë¦¼ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    def get_current_time(self):
        """í˜„ì¬ ì¬ìƒ ì‹œê°„(ì´ˆ) ë°˜í™˜"""
        if self.video_fps == 0:
            return 0
        return self.current_frame_idx / self.video_fps

    def get_total_duration(self):
        """ì „ì²´ ì˜ìƒ ê¸¸ì´(ì´ˆ) ë°˜í™˜"""
        return self.video_duration

    def read_frame(self):
        """í”„ë ˆì„ ì½ê¸° (ìŠ¤íŠ¸ë¦¬ë°)"""
        if not self.video_capture or not self.video_capture.isOpened():
            return False, None

        ret, frame = self.video_capture.read()
        if ret and frame is not None:
            if not self.frame_size_detected:
                actual_height, actual_width = frame.shape[:2]
                self.size_manager.update_video_size(actual_width, actual_height)
                self.frame_size_detected = True

            resized_frame = self.size_manager.resize_frame_if_needed(frame)
            self.current_frame_idx += 1
            return True, resized_frame
        else:
            self.logger.warning("âš ï¸ ìŠ¤íŠ¸ë¦¼ ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤.")
            return False, None

    def get_fps(self):
        return self.video_fps

    def get_progress(self):
        if self.total_frames == 0 or self.video_duration == 0:
            return 0.0
        return min(self.current_frame_idx / self.total_frames, 1.0)

    def get_window_size(self):
        return self.size_manager.get_window_size()

    def release(self):
        if self.video_capture:
            self.video_capture.release()


# ê°„ë‹¨í•œ ì°¨ì„  ê°ì§€, ì¶”ì ê¸°, ì‹ í˜¸ë“± ê°ì§€ í´ë˜ìŠ¤ë“¤
class LaneDetector:
    def __init__(self):
        self.lane_history = deque(maxlen=5)
        self.last_valid_lanes = None

    def detect_lanes(self, frame):
        height, width = frame.shape[:2]
        roi_vertices = np.array([[
            (0, height), (int(width * 0.5) - 50, int(height * 0.65)),
            (int(width * 0.5) + 50, int(height * 0.65)), (width, height)
        ]], dtype=np.int32)

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        blur = cv2.GaussianBlur(gray, (5, 5), 0)
        edges = cv2.Canny(blur, 50, 150)

        mask = np.zeros_like(edges)
        cv2.fillPoly(mask, roi_vertices, 255)
        masked_edges = cv2.bitwise_and(edges, mask)

        lines = cv2.HoughLinesP(masked_edges, rho=1, theta=np.pi / 180,
                                threshold=50, minLineLength=50, maxLineGap=50)
        return self.process_lane_lines(lines, width, height)

    def process_lane_lines(self, lines, width, height):
        if lines is None:
            return self.last_valid_lanes

        left_lines, right_lines = [], []
        for line in lines:
            x1, y1, x2, y2 = line[0]
            if x2 - x1 == 0:
                continue
            slope = (y2 - y1) / (x2 - x1)
            if slope < -0.5:
                left_lines.append((x1, y1, x2, y2))
            elif slope > 0.5:
                right_lines.append((x1, y1, x2, y2))

        left_lane = self.average_lane(left_lines, width, height)
        right_lane = self.average_lane(right_lines, width, height)

        if left_lane is not None or right_lane is not None:
            lanes = {'left': left_lane, 'right': right_lane}
            self.lane_history.append(lanes)
            self.last_valid_lanes = lanes
            return lanes

        return self.last_valid_lanes

    def average_lane(self, lane_lines, width, height):
        if not lane_lines:
            return None
        x_coords, y_coords = [], []
        for x1, y1, x2, y2 in lane_lines:
            x_coords.extend([x1, x2])
            y_coords.extend([y1, y2])
        if len(x_coords) < 2:
            return None
        poly = np.polyfit(y_coords, x_coords, 1)
        y1 = height
        y2 = int(height * 0.6)
        x1 = int(poly[0] * y1 + poly[1])
        x2 = int(poly[0] * y2 + poly[1])
        return [x1, y1, x2, y2]

    def calculate_lane_center_points(self, lanes, height):
        if not lanes or (lanes.get('left') is None and lanes.get('right') is None):
            return None
        center_points = []
        for y in range(int(height * 0.65), height, 20):
            left_x, right_x = None, None
            if lanes.get('left'):
                x1, y1, x2, y2 = lanes['left']
                if y2 != y1:
                    left_x = x1 + (x2 - x1) * (y - y1) / (y2 - y1)
            if lanes.get('right'):
                x1, y1, x2, y2 = lanes['right']
                if y2 != y1:
                    right_x = x1 + (x2 - x1) * (y - y1) / (y2 - y1)
            if left_x is not None and right_x is not None:
                center_x = (left_x + right_x) / 2
            elif left_x is not None:
                center_x = left_x + 60
            elif right_x is not None:
                center_x = right_x - 60
            else:
                continue
            center_points.append((int(center_x), y))
        return center_points


class ImprovedIOUTracker:
    def __init__(self, max_lost=5, iou_threshold=0.3, movement_threshold=2, expected_fps=30):
        self.tracks = {}
        self.next_id = 1
        self.max_lost = max_lost
        self.iou_threshold = iou_threshold
        self.movement_threshold = movement_threshold
        self.frame_count = 0
        self.expected_fps = expected_fps
        self.departure_buffer_frames = int(expected_fps * 2)
        self.stationary_threshold = 1.5
        self.min_stationary_ratio = 0.7

    @staticmethod
    def calculate_iou(box1, box2):
        x1, y1, w1, h1 = box1
        x2, y2, w2, h2 = box2
        x_left = max(x1, x2)
        y_top = max(y1, y2)
        x_right = min(x1 + w1, x2 + w2)
        y_bottom = min(y1 + h1, y2 + h2)
        if x_right < x_left or y_bottom < y_top:
            return 0.0
        intersection = (x_right - x_left) * (y_bottom - y_top)
        area1 = w1 * h1
        area2 = w2 * h2
        union = area1 + area2 - intersection
        return intersection / union if union > 0 else 0.0

    def detect_departure_improved(self, track, is_in_zone=False):
        if len(track['movement_history']) < self.departure_buffer_frames:
            return False

        recent_movements = list(track['movement_history'])[-self.departure_buffer_frames:]
        analysis_frames = recent_movements[:-15] if len(recent_movements) > 15 else recent_movements[:-5]
        if not analysis_frames:
            return False

        stationary_count = 0
        for move_x, move_y in analysis_frames:
            distance = (move_x ** 2 + move_y ** 2) ** 0.5
            if distance < self.stationary_threshold:
                stationary_count += 1

        stationary_ratio = stationary_count / len(analysis_frames)
        if stationary_ratio < self.min_stationary_ratio:
            return False

        recent_movement_frames = recent_movements[-10:] if len(recent_movements) >= 10 else recent_movements[-5:]
        avg_recent_distance = sum(
            (move_x ** 2 + move_y ** 2) ** 0.5
            for move_x, move_y in recent_movement_frames
        ) / len(recent_movement_frames)

        adjusted_threshold = self.movement_threshold * (2.0 if is_in_zone else 3.0)
        return avg_recent_distance > adjusted_threshold

    def update(self, detections):
        self.frame_count += 1
        active_tracks = {tid: tr for tid, tr in self.tracks.items() if tr['lost'] <= self.max_lost}
        matched_tracks, matched_dets = set(), set()

        for track_id, track in active_tracks.items():
            best_iou, best_idx = 0.0, -1
            for i, det in enumerate(detections):
                if i in matched_dets:
                    continue
                iou = self.calculate_iou(track['bbox'], det['box'])
                if iou > best_iou and iou > self.iou_threshold:
                    best_iou, best_idx = iou, i
            if best_idx != -1:
                old_bbox = track['bbox']
                new_bbox = detections[best_idx]['box']
                old_cx = old_bbox[0] + old_bbox[2] / 2
                old_cy = old_bbox[1] + old_bbox[3] / 2
                new_cx = new_bbox[0] + new_bbox[2] / 2
                new_cy = new_bbox[1] + new_bbox[3] / 2
                movement_x = new_cx - old_cx
                movement_y = new_cy - old_cy

                track['bbox'] = new_bbox
                track['confidence'] = detections[best_idx]['confidence']
                track['class_name'] = detections[best_idx]['class_name']
                track['lost'] = 0
                track['movement_history'].append((movement_x, movement_y))
                track['last_update'] = self.frame_count

                is_in_zone = detections[best_idx].get('in_zone', False)
                if self.detect_departure_improved(track, is_in_zone):
                    track['is_moving'] = True
                    track['departure_detected'] = True

                matched_tracks.add(track_id)
                matched_dets.add(best_idx)

        for track_id, track in active_tracks.items():
            if track_id not in matched_tracks:
                track['lost'] += 1

        max_history = max(self.departure_buffer_frames + 20, 100)
        for i, det in enumerate(detections):
            if i not in matched_dets:
                self.tracks[self.next_id] = {
                    'id': self.next_id,
                    'bbox': det['box'],
                    'confidence': det['confidence'],
                    'class_name': det['class_name'],
                    'lost': 0,
                    'created_frame': self.frame_count,
                    'last_update': self.frame_count,
                    'movement_history': deque(maxlen=max_history),
                    'is_moving': False,
                    'departure_detected': False
                }
                self.next_id += 1

        self.tracks = {tid: tr for tid, tr in self.tracks.items() if tr['lost'] <= self.max_lost}
        return self.get_active_tracks()

    def get_active_tracks(self):
        return [tr for tr in self.tracks.values() if tr['lost'] == 0]


class TrafficLightColorDetector:
    def __init__(self, stability_frames=2):
        self.stability_frames = stability_frames
        self.color_history = deque(maxlen=stability_frames)
        self.last_stable_color = None
        self.last_change_time = 0
        self.previous_color = None
        self.confidence_threshold = 0.5

    def detect_traffic_light_color(self, roi):
        if roi.size == 0:
            return None
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)

        red_lower1 = np.array([0, 100, 100]);
        red_upper1 = np.array([10, 255, 255])
        red_lower2 = np.array([170, 100, 100]);
        red_upper2 = np.array([180, 255, 255])
        yellow_lower = np.array([18, 120, 120]);
        yellow_upper = np.array([35, 255, 255])
        green_lower = np.array([45, 100, 100]);
        green_upper = np.array([90, 255, 255])

        red_mask = cv2.inRange(hsv, red_lower1, red_upper1) + cv2.inRange(hsv, red_lower2, red_upper2)
        yellow_mask = cv2.inRange(hsv, yellow_lower, yellow_upper)
        green_mask = cv2.inRange(hsv, green_lower, green_upper)

        red_pixels = cv2.countNonZero(red_mask)
        yellow_pixels = cv2.countNonZero(yellow_mask)
        green_pixels = cv2.countNonZero(green_mask)
        max_pixels = max(red_pixels, yellow_pixels, green_pixels)

        if max_pixels < 15:
            return None
        if red_pixels == max_pixels:
            return 'red'
        elif yellow_pixels == max_pixels:
            return 'yellow'
        elif green_pixels == max_pixels:
            return 'green'
        return None

    def update_color_history(self, color):
        if color:
            self.color_history.append(color)
            if len(self.color_history) >= self.stability_frames:
                counts = {}
                for c in self.color_history:
                    counts[c] = counts.get(c, 0) + 1
                most_common = max(counts, key=counts.get)
                confidence = counts[most_common] / self.stability_frames
                if confidence >= self.confidence_threshold:
                    if self.last_stable_color != most_common:
                        change_info = {
                            'datetime': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            'timestamp': time.time()
                        }
                        self.previous_color = self.last_stable_color
                        self.last_stable_color = most_common
                        self.last_change_time = time.time()
                        return change_info
        return None


class YouTubeBlackBoxSystem:
    """âœ… í•œê¸€ í°íŠ¸ ê¹¨ì§ ìˆ˜ì • + YouTube ìŠ¤íŠ¸ë¦¼ ì „ìš© ìµœì í™” ì‹œìŠ¤í…œ"""

    def __init__(self, config_path="youtube_blackbox_clean.json"):
        self.setup_logging()
        self.config_path = config_path
        self.config = self.load_config()

        self.video_manager = YouTubeVideoManager(self.logger, self.config)
        # âœ… í•œê¸€ í…ìŠ¤íŠ¸ ë Œë”ëŸ¬ ì¶”ê°€
        self.text_renderer = KoreanTextRenderer()

        self.net = None
        self.classes = []
        self.output_layers = []

        self.running = False
        self.current_frame = None
        self.paused = False

        expected_fps = 30
        self.vehicle_tracker = ImprovedIOUTracker(
            max_lost=int(self.config.get('tracking', {}).get('max_lost_frames', 8)),
            iou_threshold=float(self.config.get('tracking', {}).get('iou_threshold', 0.25)),
            movement_threshold=float(self.config.get('tracking', {}).get('movement_threshold', 2)),
            expected_fps=expected_fps
        )
        self.traffic_light_detector = TrafficLightColorDetector(
            stability_frames=int(self.config.get('traffic_light', {}).get('stability_frames', 2))
        )
        self.lane_detector = LaneDetector()

        self.detection_stats = {
            'total_detections': 0,
            'vehicles': 0,
            'traffic_lights': 0,
            'traffic_light_changes': 0,
            'vehicle_departures': 0
        }

        self.processing_times = deque(maxlen=30)

        self.logger.info("âœ… YouTube ìŠ¤íŠ¸ë¦¼ ì „ìš© ë¸”ë™ë°•ìŠ¤ ì‹œìŠ¤í…œ (í•œê¸€ ì§€ì›) ì´ˆê¸°í™” ì™„ë£Œ")

        # TTS ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        try:
            self.tts_system = TTSNavigationSystem()
            self.TTS_ALLOWED = ["ì‹ í˜¸ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤", "ì•ì˜ ì°¨ëŸ‰ì´ ì¶œë°œí•˜ì˜€ìŠµë‹ˆë‹¤"]
            self.tts_message_hashes = set()
        except Exception as e:
            self.logger.warning(f"TTS ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.tts_system = None

    def setup_logging(self):
        os.makedirs("log", exist_ok=True)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('log/youtube_clean.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def load_config(self):
        default_config = {
            "youtube": {
                "url": "https://www.youtube.com/watch?v=OoHzud9L48s",
                "quality": "best",
                "output_dir": "output"
            },
            "model": {
                "weights_path": "yolov4-tiny.weights",
                "config_path": "yolov4-tiny.cfg",
                "classes_path": "coco.names",
                "input_size": 416
            },
            "detection": {
                "confidence_threshold": 0.5,
                "nms_threshold": 0.4,
                "traffic_light_confidence_threshold": 0.3,
                "detection_interval": 2
            },
            "traffic_light": {
                "enable_detection": True,
                "stability_frames": 2
            },
            "tracking": {
                "iou_threshold": 0.25,
                "max_lost_frames": 8,
                "movement_threshold": 2
            },
            "display": {
                "show_preview": True,
                "auto_size": True
            }
        }

        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    cfg = json.load(f)

                def merge(a, b):
                    for k, v in b.items():
                        if k not in a:
                            a[k] = v
                        elif isinstance(v, dict):
                            merge(a[k], v)
                    return a

                return merge(cfg, default_config)
            else:
                with open(self.config_path, 'w', encoding='utf-8') as f:
                    json.dump(default_config, f, indent=2, ensure_ascii=False)
                return default_config
        except Exception as e:
            self.logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return default_config

    def get_youtube_url_from_user(self):
        """ì‚¬ìš©ìë¡œë¶€í„° YouTube URL ì…ë ¥ë°›ê¸°"""
        print("\n" + "=" * 80)
        print("     YouTube Stream BlackBox System (Clean Korean UI)")
        print("=" * 80)

        default_url = self.config.get('youtube', {}).get('url', '')
        if default_url:
            print(f"ê¸°ë³¸ URL: {default_url}")

        while True:
            try:
                user_input = input("\nYouTube URLì„ ì…ë ¥í•˜ì„¸ìš” (Enter=ê¸°ë³¸ê°’ ì‚¬ìš©): ").strip()

                if not user_input and default_url:
                    youtube_url = default_url
                elif not user_input:
                    print("âŒ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                else:
                    youtube_url = user_input

                if 'youtube.com' in youtube_url or 'youtu.be' in youtube_url:
                    self.logger.info(f"âœ… YouTube URL í™•ì¸: {youtube_url}")
                    return youtube_url
                else:
                    print("âŒ ì˜¬ë°”ë¥¸ YouTube URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

            except KeyboardInterrupt:
                print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return None
            except Exception as e:
                print(f"âŒ ì…ë ¥ ì˜¤ë¥˜: {e}")

    def init_ai_model(self):
        try:
            weights_path = self.config['model']['weights_path']
            config_path = self.config['model']['config_path']
            classes_path = self.config['model']['classes_path']

            if not all(os.path.exists(p) for p in [weights_path, config_path, classes_path]):
                self.logger.warning("AI ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì²˜ë¦¬ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
                return False

            try:
                self.net = cv2.dnn.readNetFromDarknet(config_path, weights_path)
            except Exception:
                self.net = cv2.dnn.readNet(weights_path, config_path)

            self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
            self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)

            with open(classes_path, 'r', encoding='utf-8') as f:
                self.classes = [line.strip() for line in f if line.strip()]

            layer_names = self.net.getLayerNames()
            unconnected = self.net.getUnconnectedOutLayers()
            self.output_layers = [layer_names[i - 1] for i in unconnected]

            self.logger.info(f"âœ… AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {len(self.classes)}ê°œ í´ë˜ìŠ¤")
            return True

        except Exception as e:
            self.logger.error(f"AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def filter_vehicles_by_lane(self, vehicle_detections, frame_width, frame_height, frame):
        """ì°¨ì„  ê¸°ë°˜ ì•ì°¨ í•„í„°ë§"""
        lanes = self.lane_detector.detect_lanes(frame)
        if lanes is None:
            return []

        center_points = self.lane_detector.calculate_lane_center_points(lanes, frame_height)
        if not center_points:
            return []

        filtered = []
        lane_width = 120
        min_area = (frame_width * frame_height) * 0.008

        for det in vehicle_detections:
            if det['class_name'] not in ['car', 'truck', 'bus', 'motorbike', 'bicycle']:
                continue

            x, y, w, h = det['box']
            vehicle_center_x = x + w / 2
            vehicle_center_y = y + h / 2

            if vehicle_center_y < frame_height * 0.6:
                continue

            closest_point = min(center_points, key=lambda p: abs(p[1] - vehicle_center_y))
            lane_center_x = closest_point[0]

            if abs(vehicle_center_x - lane_center_x) < lane_width / 2 and (w * h) > min_area:
                det['in_zone'] = True
                det['front_vehicle'] = True
                filtered.append(det)

        return filtered

    def detect_objects_optimized(self, frame):
        """ì„±ëŠ¥ ìµœì í™”ëœ ê°ì²´ ê°ì§€"""
        start_time = time.time()

        height, width = frame.shape[:2]
        input_size = int(self.config['model'].get('input_size', 416))

        blob = cv2.dnn.blobFromImage(frame, 0.00392, (input_size, input_size), (0, 0, 0), True, crop=False)
        self.net.setInput(blob)
        outputs = self.net.forward(self.output_layers)

        boxes, confidences, class_ids = [], [], []
        conf_th = float(self.config['detection']['confidence_threshold'])
        tl_conf_th = float(self.config['detection'].get('traffic_light_confidence_threshold', conf_th))
        nms_th = float(self.config['detection']['nms_threshold'])

        for output in outputs:
            for detection in output:
                scores = detection[5:]
                if scores.size == 0:
                    continue
                class_id = int(np.argmax(scores))
                if class_id < 0 or class_id >= len(self.classes):
                    continue
                confidence = float(scores[class_id])
                class_name = self.classes[class_id]
                min_conf = tl_conf_th if class_name == 'traffic light' else conf_th
                if confidence > min_conf:
                    cx = int(detection[0] * width)
                    cy = int(detection[1] * height)
                    w = int(detection[2] * width)
                    h = int(detection[3] * height)
                    x = int(cx - w / 2)
                    y = int(cy - h / 2)
                    boxes.append([x, y, w, h])
                    confidences.append(confidence)
                    class_ids.append(class_id)

        indexes = cv2.dnn.NMSBoxes(boxes, confidences, conf_th, nms_th)
        detections, vehicle_dets, tl_dets = [], [], []

        if len(indexes) > 0:
            for i in indexes.flatten():
                class_name = self.classes[class_ids[i]]
                det = {
                    'class_name': class_name,
                    'confidence': float(confidences[i]),
                    'box': boxes[i],
                    'timestamp': time.time()
                }
                detections.append(det)

                if class_name == 'traffic light':
                    tl_dets.append(det)
                    self.detection_stats['traffic_lights'] += 1

                if class_name in ['car', 'truck', 'bus', 'motorbike', 'bicycle']:
                    vehicle_dets.append(det)
                    self.detection_stats['vehicles'] += 1

            self.detection_stats['total_detections'] += len(indexes)

        processing_time = time.time() - start_time
        self.processing_times.append(processing_time)

        filtered_vehicle_dets = self.filter_vehicles_by_lane(vehicle_dets, width, height, frame)
        tracked_vehicles = self.vehicle_tracker.update(filtered_vehicle_dets) if filtered_vehicle_dets else []

        if tl_dets and self.config['traffic_light']['enable_detection']:
            self.analyze_traffic_light_colors_fast(frame, tl_dets)

        return detections, tracked_vehicles

    def analyze_traffic_light_colors_fast(self, frame, tl_detections):
        for det in tl_detections:
            x, y, w, h = det['box']
            roi = frame[max(0, y):min(frame.shape[0], y + h),
                  max(0, x):min(frame.shape[1], x + w)]
            if roi.size > 100:
                detected_color = self.traffic_light_detector.detect_traffic_light_color(roi)
                change = self.traffic_light_detector.update_color_history(detected_color)
                if change:
                    self.handle_traffic_light_change(change)

    def handle_traffic_light_change(self, change_info):
        log_message = "ì‹ í˜¸ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤"
        self.logger.info(f"ğŸš¦ {log_message}")
        self._tts_announce_if_needed(log_message)
        self.detection_stats['traffic_light_changes'] += 1

    def log_vehicle_departure(self, track):
        if track.get('departure_detected', False):
            log_message = "ì•ì˜ ì°¨ëŸ‰ì´ ì¶œë°œí•˜ì˜€ìŠµë‹ˆë‹¤"
            self.logger.info(f"ğŸš— ì•ì°¨ ì¶œë°œ: ID{track['id']} ({track['class_name']})")
            self.detection_stats['vehicle_departures'] += 1
            self._tts_announce_if_needed(log_message)

    def _tts_announce_if_needed(self, msg):
        if not self.tts_system:
            return
        msg_hash = hashlib.md5(msg.encode()).hexdigest()
        if msg in self.TTS_ALLOWED and msg_hash not in self.tts_message_hashes:
            try:
                self.tts_system.play_situation_from_txt(msg, keyword=msg)
                self.tts_message_hashes.add(msg_hash)
                if len(self.tts_message_hashes) > 1000:
                    self.tts_message_hashes.pop()
            except Exception as e:
                self.logger.error(f"TTS ì•ˆë‚´ ì‹¤íŒ¨: {e}")

    def format_time(self, seconds):
        """ì‹œê°„ í¬ë§·íŒ…"""
        if seconds < 0:
            return "00:00"
        hours = int(seconds // 3600)
        minutes = int(seconds % 3600) // 60
        secs = int(seconds % 60)
        if hours > 0:
            return f"{hours:02d}:{minutes:02d}:{secs:02d}"
        else:
            return f"{minutes:02d}:{secs:02d}"

    def draw_clean_overlay(self, frame, detections, tracked_vehicles):
        """âœ… ê¹”ë”í•˜ê³  í•œê¸€ì´ ì™„ë²½í•˜ê²Œ í‘œì‹œë˜ëŠ” ì˜¤ë²„ë ˆì´ (ì‹œê°„ ì¡°ì ˆ UI ì œê±°)"""
        overlay = frame.copy()
        height, width = frame.shape[:2]

        # ì°¨ì„  ì˜¤ë²„ë ˆì´
        lanes = self.lane_detector.last_valid_lanes
        center_points = self.lane_detector.calculate_lane_center_points(lanes, height) if lanes else None

        if lanes:
            if lanes.get('left'):
                x1, y1, x2, y2 = lanes['left']
                cv2.line(overlay, (x1, y1), (x2, y2), (0, 255, 0), 2)
            if lanes.get('right'):
                x1, y1, x2, y2 = lanes['right']
                cv2.line(overlay, (x1, y1), (x2, y2), (0, 255, 0), 2)
            if center_points:
                for i in range(len(center_points) - 1):
                    cv2.line(overlay, center_points[i], center_points[i + 1], (0, 255, 255), 3)

        # ê²€ì¶œëœ ì°¨ëŸ‰ë“¤ í‘œì‹œ
        for detection in detections:
            x, y, w, h = detection['box']
            class_name = detection['class_name']
            confidence = detection['confidence']

            if class_name in ['car', 'truck', 'bus', 'motorbike', 'bicycle']:
                vehicle_center_y = y + h / 2
                if vehicle_center_y > height * 0.6:
                    colors = {
                        'car': (0, 255, 0),
                        'truck': (0, 255, 255),
                        'bus': (255, 255, 0),
                        'motorbike': (255, 0, 255),
                        'bicycle': (100, 255, 255)
                    }
                    color = colors.get(class_name, (0, 255, 0))
                    cv2.rectangle(overlay, (x, y), (x + w, y + h), color, 2)

                    conf_text = f"{class_name}: {confidence:.2f}"
                    self.text_renderer.put_korean_text(overlay, conf_text, (x, y - 5),
                                                       font_scale=0.4, color=color, thickness=1)

            elif class_name == 'traffic light':
                cv2.rectangle(overlay, (x, y), (x + w, y + h), (255, 255, 255), 2)
                current_color = self.traffic_light_detector.last_stable_color
                if current_color:
                    signal_colors = {'red': (0, 0, 255), 'yellow': (0, 255, 255), 'green': (0, 255, 0)}
                    signal_color = signal_colors.get(current_color, (255, 255, 255))
                    cv2.circle(overlay, (x + w // 2, y - 15), 8, signal_color, -1)

        # ì¶”ì ëœ ì•ì°¨ í‘œì‹œ (í•œê¸€ ì™„ë²½ ì§€ì›)
        for tr in tracked_vehicles:
            x, y, w, h = tr['bbox']
            track_id = tr['id']

            if tr.get('departure_detected', False):
                track_color = (0, 255, 255)
                status_text = "DEPARTED"
            elif tr.get('is_moving', False):
                track_color = (255, 255, 0)
                status_text = "MOVING"
            else:
                track_color = (0, 255, 0)
                status_text = "WAITING"

            cv2.rectangle(overlay, (x, y), (x + w, y + h), track_color, 3)

            id_text = f"ID-{track_id} ({status_text})"
            self.text_renderer.put_korean_text(overlay, id_text, (x + 5, y - 5),
                                               font_scale=0.5, color=track_color, thickness=1,
                                               background=(0, 0, 0))

        # âœ… ê°„ì†Œí™”ëœ ìƒë‹¨ íŒ¨ë„ (í•œê¸€ ì™„ë²½ ì§€ì›)
        panel_h = 60
        panel_overlay = overlay.copy()
        cv2.rectangle(panel_overlay, (0, 0), (width, panel_h), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.8, panel_overlay, 0.2, 0, overlay)
        cv2.line(overlay, (0, panel_h), (width, panel_h), (0, 255, 100), 2)

        # YouTube ì •ë³´ í‘œì‹œ
        title = self.video_manager.video_info.get('title', 'YouTube Stream')
        youtube_info = f"ğŸ“º {title[:40]}{'...' if len(title) > 40 else ''}"
        self.text_renderer.put_korean_text(overlay, youtube_info, (15, 20),
                                           font_scale=0.5, color=(255, 255, 255), thickness=1)

        # ìŠ¤íŠ¸ë¦¼ ì‹œê°„ ì •ë³´ (ì‹œê°„ ì¡°ì ˆ UIëŠ” ì œê±°í•˜ê³  ì •ë³´ë§Œ í‘œì‹œ)
        current_time = self.video_manager.get_current_time()
        total_time = self.video_manager.get_total_duration()
        if total_time > 0:
            time_text = f"TIME: {self.format_time(current_time)} / {self.format_time(total_time)}"
        else:
            time_text = f"LIVE TIME: {self.format_time(current_time)}"
        self.text_renderer.put_korean_text(overlay, time_text, (15, 40),
                                           font_scale=0.4, color=(255, 255, 255), thickness=1)

        # ì•ì°¨ ê°ì§€ ìƒíƒœ (í•œê¸€ â†’ ì˜ë¬¸ ë³€í™˜ìœ¼ë¡œ í‘œì‹œ)
        waiting_vehicles = len([t for t in tracked_vehicles if not t.get('is_moving', False)])
        moving_vehicles = len([t for t in tracked_vehicles if t.get('is_moving', False)])
        departed_vehicles = len([t for t in tracked_vehicles if t.get('departure_detected', False)])

        status_text = f"ğŸš— WAIT:{waiting_vehicles} MOVE:{moving_vehicles} DEPT:{departed_vehicles}"
        self.text_renderer.put_korean_text(overlay, status_text, (width - 300, 25),
                                           font_scale=0.4, color=(0, 255, 0), thickness=1)

        # ì‹ í˜¸ë“± ìƒíƒœ
        if self.traffic_light_detector.last_stable_color:
            signal_text = f"ğŸš¦ SIGNAL: {self.traffic_light_detector.last_stable_color.upper()}"
            self.text_renderer.put_korean_text(overlay, signal_text, (width - 300, 45),
                                               font_scale=0.4, color=(0, 255, 255), thickness=1)

        # ì¼ì‹œì •ì§€ ìƒíƒœ
        if self.paused:
            pause_text = "â¸ï¸ PAUSED"
            self.text_renderer.put_korean_text(overlay, pause_text, (width // 2 - 40, height // 2),
                                               font_scale=1, color=(0, 0, 255), thickness=2)

        # ìŠ¤íŠ¸ë¦¼ ì—°ê²° ìƒíƒœ
        cv2.circle(overlay, (width - 30, 30), 8, (0, 255, 0), -1)
        self.text_renderer.put_korean_text(overlay, "LIVE", (width - 60, 35),
                                           font_scale=0.3, color=(0, 255, 0), thickness=1)

        # âœ… ê°„ì†Œí™”ëœ ì»¨íŠ¸ë¡¤ íŒíŠ¸ (ì‹œê°„ ì¡°ì ˆ ê´€ë ¨ ì œê±°)
        control_text = "CONTROLS: [SPACE] Pause [S] Screenshot [Q] Quit [H] Help"
        self.text_renderer.put_korean_text(overlay, control_text, (10, height - 10),
                                           font_scale=0.4, color=(200, 200, 200), thickness=1)

        return overlay

    def handle_keyboard_input(self, key):
        """âœ… í‚¤ë³´ë“œ ì…ë ¥ ì²˜ë¦¬ (ì‹œê°„ ì¡°ì ˆ ê´€ë ¨ ì œê±°)"""
        if key == -1:
            return False

        if key == ord('q') or key == ord('Q'):
            self.logger.info("ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            self.running = False
            return True

        elif key == ord('s') or key == ord('S'):
            screenshot_name = f"youtube_clean_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg"
            if self.current_frame is not None:
                cv2.imwrite(screenshot_name, self.current_frame)
                self.logger.info(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_name}")

        elif key == 32:  # SPACE
            self.paused = not self.paused
            status = "ì¼ì‹œì •ì§€" if self.paused else "ì¬ê°œ"
            self.logger.info(f"â¸ï¸ ìŠ¤íŠ¸ë¦¼ {status}")

        elif key == ord('h') or key == ord('H'):
            self.show_help()

        # âœ… ì‹œê°„ ì´ë™ ê´€ë ¨ í‚¤ëŠ” ëª¨ë‘ ì œê±°í•˜ê³  ì•ˆë‚´ ë©”ì‹œì§€ë§Œ í‘œì‹œ
        elif key in [2424832, 65361, 2555904, 65363, 2490368, 65362, 2621440, 65364]:
            self.logger.info("â„¹ï¸ YouTube ìŠ¤íŠ¸ë¦¼ì—ì„œëŠ” ì‹œê°„ ì´ë™ ê¸°ëŠ¥ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        return False

    def show_help(self):
        """âœ… ê°„ì†Œí™”ëœ ë„ì›€ë§ (ì‹œê°„ ì¡°ì ˆ ê´€ë ¨ ì œê±°)"""
        help_text = """
==================== YouTube Stream BlackBox (Clean UI) ====================
[ê¸°ë³¸ ì œì–´]
Q - í”„ë¡œê·¸ë¨ ì¢…ë£Œ
SPACE - ì¼ì‹œì •ì§€/ì¬ê°œ  
S - ìŠ¤í¬ë¦°ìƒ· ì €ì¥
H - ì´ ë„ì›€ë§ í‘œì‹œ

[íŠ¹ì§•]
- í•œê¸€ í…ìŠ¤íŠ¸ ì™„ë²½ ì§€ì› (ë¬¼ìŒí‘œ ì—†ìŒ)
- YouTube ìŠ¤íŠ¸ë¦¼ ì „ìš© ìµœì í™”
- ë¶ˆí•„ìš”í•œ ì‹œê°„ ì¡°ì ˆ UI ì œê±°
- ê¹”ë”í•˜ê³  ì§ê´€ì ì¸ ì¸í„°í˜ì´ìŠ¤

[ê¸°ëŠ¥]
- ì‹¤ì‹œê°„ ì•ì°¨ ì¶œë°œ ê°ì§€
- ì‹ í˜¸ë“± ìƒíƒœ ì¸ì‹
- ì°¨ì„  ê¸°ë°˜ ì •ë°€ ì¶”ì 
- ìë™ ì˜ìƒ í¬ê¸° ì¡°ì •

[ì œí•œì‚¬í•­]
- YouTube ìŠ¤íŠ¸ë¦¼ íŠ¹ì„±ìƒ ì‹œê°„ ì´ë™(ë˜ê°ê¸°/ë¹¨ë¦¬ê°ê¸°) ë¶ˆê°€
- ë„¤íŠ¸ì›Œí¬ ìƒíƒœì— ë”°ë¥¸ ì§€ì—° ê°€ëŠ¥
========================================================================
"""
        self.logger.info(help_text)
        print(help_text)

    def run(self):
        # ì‚¬ìš©ìë¡œë¶€í„° YouTube URL ì…ë ¥ë°›ê¸°
        youtube_url = self.get_youtube_url_from_user()
        if not youtube_url:
            return False

        # YouTube ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™”
        if not self.video_manager.init_youtube_video(youtube_url):
            return False

        ai_ok = self.init_ai_model()
        if not ai_ok:
            self.logger.warning("AI ëª¨ë¸ ì—†ì´ ê¸°ë³¸ ì²˜ë¦¬ ëª¨ë“œë¡œ ì‹¤í–‰")

        self.running = True
        frame_count = 0

        # ì‹¤ì œ ì˜ìƒ í¬ê¸°ì— ë§ê²Œ ìœˆë„ìš° ì„¤ì •
        if self.config['display'].get('show_preview', True):
            window_name = "YouTube Stream BlackBox (Clean Korean UI)"
            cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)

            display_w, display_h = self.video_manager.get_window_size()
            cv2.resizeWindow(window_name, display_w, display_h)

            self.logger.info(f"âœ… ìœˆë„ìš° í¬ê¸° ìë™ ì„¤ì •: {display_w}x{display_h}")

        video_fps = self.video_manager.get_fps()
        frame_delay = 1.0 / video_fps if video_fps > 0 else 1.0 / 30

        self.logger.info(f"ğŸ¬ YouTube ìŠ¤íŠ¸ë¦¼ ì‹œì‘ (FPS: {video_fps:.1f})")
        self.logger.info("âœ… í•œê¸€ ì™„ë²½ ì§€ì› + ê¹”ë”í•œ UI")
        self.logger.info("âœ… ì»¨íŠ¸ë¡¤: [SPACE]ì¼ì‹œì •ì§€ [S]ìŠ¤í¬ë¦°ìƒ· [H]ë„ì›€ë§ [Q]ì¢…ë£Œ")

        try:
            while self.running:
                if not self.paused:
                    ret, frame = self.video_manager.read_frame()
                    if not ret or frame is None:
                        self.logger.warning("âš ï¸ ìŠ¤íŠ¸ë¦¼ ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤. ì¬ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤...")
                        time.sleep(5)
                        continue

                    self.current_frame = frame.copy()

                    detections, tracked_vehicles = [], []
                    detection_interval = int(self.config['detection']['detection_interval'])

                    if ai_ok and self.net is not None:
                        if frame_count % detection_interval == 0:
                            detections, tracked_vehicles = self.detect_objects_optimized(frame)

                    # âœ… ê¹”ë”í•œ ì˜¤ë²„ë ˆì´ ì ìš© (í•œê¸€ ì™„ë²½ ì§€ì›)
                    overlay_frame = self.draw_clean_overlay(frame.copy(), detections, tracked_vehicles)

                    # ì•ì°¨ ì¶œë°œ ê°ì§€ ë¡œê·¸
                    for track in tracked_vehicles:
                        if track.get('departure_detected', False) and not track.get('logged', False):
                            self.log_vehicle_departure(track)
                            track['logged'] = True

                    frame_count += 1

                else:
                    # ì¼ì‹œì •ì§€ ìƒíƒœ
                    if self.current_frame is not None:
                        overlay_frame = self.draw_clean_overlay(self.current_frame.copy(), [], [])
                    else:
                        time.sleep(0.1)
                        continue

                # ì‹¤ì‹œê°„ ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ
                if self.config['display'].get('show_preview', True):
                    cv2.imshow(window_name, overlay_frame)

                    # âœ… waitKeyEx()ë¡œ í‚¤ ì…ë ¥ ì²˜ë¦¬ (í™”ì‚´í‘œ í‚¤ ì§€ì› ìœ ì§€)
                    key = cv2.waitKeyEx(1)

                    if self.handle_keyboard_input(key):
                        break

                # ìŠ¤íŠ¸ë¦¼ ì†ë„ ì¡°ì ˆ
                if not self.paused:
                    time.sleep(frame_delay)

        except KeyboardInterrupt:
            self.logger.info("Keyboard interrupt ê°ì§€")
        except Exception as e:
            self.logger.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            self.cleanup_resources()

        return True

    def cleanup_resources(self):
        try:
            self.video_manager.release()
            cv2.destroyAllWindows()
            if hasattr(self, 'tts_system') and self.tts_system:
                self.tts_system.shutdown()
        except Exception as e:
            self.logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")


def main():
    parser = argparse.ArgumentParser(description='YouTube Stream BlackBox (Clean Korean UI)')
    parser.add_argument('--config', default='youtube_blackbox_clean.json', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--url', help='YouTube URL (ëŒ€í™”í˜• ì…ë ¥ ìƒëµ)')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ëª¨ë“œ')
    args = parser.parse_args()

    if not YT_DLP_AVAILABLE:
        print("âŒ yt-dlp ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install yt-dlp")
        return 1

    try:
        blackbox = YouTubeBlackBoxSystem(args.config)

        if args.url:
            blackbox.config['youtube']['url'] = args.url

        success = blackbox.run()
        return 0 if success else 1
    except KeyboardInterrupt:
        return 0
    except Exception as e:
        if args.debug:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
